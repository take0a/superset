---
title: データベースへの接続
hide_title: true
sidebar_position: 1
version: 1
---
# データベースへの接続

Superset には、データベースへの接続機能がバンドルされていません。
Superset をデータベースに接続するための主な手順は、**適切なデータベース ドライバーを環境にインストールする** ことです。

:::note
メタデータ データベースとして使用するデータベースに必要なパッケージと、Superset 経由でアクセスするデータベースに接続するために必要なパッケージをインストールする必要があります。
Superset のメタデータ データベースの設定については、インストール ドキュメント ([Docker Compose](/docs/installation/docker-compose)、[Kubernetes](/docs/installation/kubernetes)) を参照してください。
:::

このドキュメントでは、一般的に使用されるデータベース エンジンのさまざまなドライバーへのポインターを維持しようとします。

## データベースドライバーのインストール

Superset では、接続するデータベース エンジンごとに、Python [DB-API データベース ドライバー](https://peps.python.org/pep-0249/) と [SQLAlchemy 方言](https://docs.sqlalchemy.org/en/20/dialects/) がインストールされている必要があります。

Superset 構成に新しいデータベース ドライバーをインストールする方法については、[こちら](/docs/configuration/databases#installing-drivers-in-docker-images) で詳しく読むことができます。

### サポートされているデータベースと依存関係

推奨パッケージの一部を以下に示します。Supersetと互換性のあるバージョンについては、[pyproject.toml](https://github.com/apache/superset/blob/master/pyproject.toml) を参照してください。

| <div style={{width: '150px'}}>Database</div>                   | PyPI package                                                                       | Connection String                                                                                                                                      |
| --------------------------------------------------------- | ---------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [AWS Athena](/docs/configuration/databases#aws-athena)                   | `pip install pyathena[pandas]` , `pip install PyAthenaJDBC`                        | `awsathena+rest://{access_key_id}:{access_key}@athena.{region}.amazonaws.com/{schema}?s3_staging_dir={s3_staging_dir}&...`   |
| [AWS DynamoDB](/docs/configuration/databases#aws-dynamodb)               | `pip install pydynamodb`                                                           | `dynamodb://{access_key_id}:{secret_access_key}@dynamodb.{region_name}.amazonaws.com?connector=superset`                                               |
| [AWS Redshift](/docs/configuration/databases#aws-redshift)               | `pip install sqlalchemy-redshift`                                                  | `redshift+psycopg2://<userName>:<DBPassword>@<AWS End Point>:5439/<Database Name>`                                                                    |
| [Apache Doris](/docs/configuration/databases#apache-doris)                     | `pip install pydoris`                                                              | `doris://<User>:<Password>@<Host>:<Port>/<Catalog>.<Database>`                                            |
| [Apache Drill](/docs/configuration/databases#apache-drill)                     | `pip install sqlalchemy-drill`                                                     | `drill+sadrill://<username>:<password>@<host>:<port>/<storage_plugin>`, often useful: `?use_ssl=True/False`                                                                                                              |
| [Apache Druid](/docs/configuration/databases#apache-druid)                     | `pip install pydruid`                                                              | `druid://<User>:<password>@<Host>:<Port-default-9088>/druid/v2/sql`                                                                                    |
| [Apache Hive](/docs/configuration/databases#hive)                       | `pip install pyhive`                                                               | `hive://hive@{hostname}:{port}/{database}`                                                                                                             |
| [Apache Impala](/docs/configuration/databases#apache-impala)                   | `pip install impyla`                                                               | `impala://{hostname}:{port}/{database}`                                                                                                                |
| [Apache Kylin](/docs/configuration/databases#apache-kylin)                     | `pip install kylinpy`                                                              | `kylin://<username>:<password>@<hostname>:<port>/<project>?<param1>=<value1>&<param2>=<value2>`                                                        |
| [Apache Pinot](/docs/configuration/databases#apache-pinot)                     | `pip install pinotdb`                                                              | `pinot://BROKER:5436/query?server=http://CONTROLLER:5983/`                                                                                             |
| [Apache Solr](/docs/configuration/databases#apache-solr)                       | `pip install sqlalchemy-solr`                                                      | `solr://{username}:{password}@{hostname}:{port}/{server_path}/{collection}`                                                                            |
| [Apache Spark SQL](/docs/configuration/databases#apache-spark-sql)             | `pip install pyhive`                                                               | `hive://hive@{hostname}:{port}/{database}`                                                                                                             |
| [Ascend.io](/docs/configuration/databases#ascendio)                       | `pip install impyla`                                                               | `ascend://{username}:{password}@{hostname}:{port}/{database}?auth_mechanism=PLAIN;use_ssl=true`                                                        |
| [Azure MS SQL](/docs/configuration/databases#sql-server)                | `pip install pymssql`                                                              | `mssql+pymssql://UserName@presetSQL:TestPassword@presetSQL.database.windows.net:1433/TestSchema`                                                       |
| [ClickHouse](/docs/configuration/databases#clickhouse)                  | `pip install clickhouse-connect`                                                   | `clickhousedb://{username}:{password}@{hostname}:{port}/{database}`                                                                                    |
| [CockroachDB](/docs/configuration/databases#cockroachdb)                | `pip install cockroachdb`                                                          | `cockroachdb://root@{hostname}:{port}/{database}?sslmode=disable`                                                                                      |
| [Couchbase](/docs/configuration/databases#couchbase)                | `pip install couchbase-sqlalchemy`                                                          | `couchbase://{username}:{password}@{hostname}:{port}?truststorepath={ssl certificate path}`                                                                                      |
| [CrateDB](/docs/configuration/databases#cratedb)                        | `pip install sqlalchemy-cratedb`                                                   | `crate://{username}:{password}@{hostname}:{port}`, often useful: `?ssl=true/false` or `?schema=testdrive`.                                             |
| [Denodo](/docs/configuration/databases#denodo)                          | `pip install denodo-sqlalchemy`                                                    | `denodo://{username}:{password}@{hostname}:{port}/{database}`                                                                                          |
| [Dremio](/docs/configuration/databases#dremio)                          | `pip install sqlalchemy_dremio`                                                    |`dremio+flight://{username}:{password}@{host}:32010`, often useful: `?UseEncryption=true/false`. For Legacy ODBC: `dremio+pyodbc://{username}:{password}@{host}:31010`                                                                                                                           |
| [Elasticsearch](/docs/configuration/databases#elasticsearch)            | `pip install elasticsearch-dbapi`                                                  | `elasticsearch+http://{user}:{password}@{host}:9200/`                                                                                                  |
| [Exasol](/docs/configuration/databases#exasol)                          | `pip install sqlalchemy-exasol`                                                    | `exa+pyodbc://{username}:{password}@{hostname}:{port}/my_schema?CONNECTIONLCALL=en_US.UTF-8&driver=EXAODBC`                                            |
| [Google BigQuery](/docs/configuration/databases#google-bigquery)                     | `pip install sqlalchemy-bigquery`                                                  | `bigquery://{project_id}`                                                                                                                              |
| [Google Sheets](/docs/configuration/databases#google-sheets)            | `pip install shillelagh[gsheetsapi]`                                               | `gsheets://`                                                                                                                                           |
| [Firebolt](/docs/configuration/databases#firebolt)                      | `pip install firebolt-sqlalchemy`                                                  | `firebolt://{client_id}:{client_secret}@{database}/{engine_name}?account_name={name}`                                                                  |
| [Hologres](/docs/configuration/databases#hologres)                      | `pip install psycopg2`                                                             | `postgresql+psycopg2://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                        |
| [IBM Db2](/docs/configuration/databases#ibm-db2)                        | `pip install ibm_db_sa`                                                            | `db2+ibm_db://`                                                                                                                                        |
| [IBM Netezza Performance Server](/docs/configuration/databases#ibm-netezza-performance-server) | `pip install nzalchemy`                                                            | `netezza+nzpy://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                               |
| [MySQL](/docs/configuration/databases#mysql)                            | `pip install mysqlclient`                                                          | `mysql://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                                      |
| [OceanBase](/docs/configuration/databases#oceanbase)                    | `pip install oceanbase_py`                                                         | `oceanbase://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                                      |
| [Oracle](/docs/configuration/databases#oracle)                          | `pip install cx_Oracle`                                                            | `oracle://<username>:<password>@<hostname>:<port>`                                                                                                                                            |
| [Parseable](/docs/configuration/databases#parseable)                    | `pip install sqlalchemy-parseable`                                                 | `parseable://<UserName>:<DBPassword>@<Database Host>/<Stream Name>`                                                                                    |
| [PostgreSQL](/docs/configuration/databases#postgres)                    | `pip install psycopg2`                                                             | `postgresql://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                                 |
| [Presto](/docs/configuration/databases#presto)                          | `pip install pyhive`                                                               | `presto://{username}:{password}@{hostname}:{port}/{database}`                                                                                                                                            |
| [Rockset](/docs/configuration/databases#rockset)                        | `pip install rockset-sqlalchemy`                                                   | `rockset://<api_key>:@<api_server>`                                                                                                                    |
| [SAP Hana](/docs/configuration/databases#hana)                          | `pip install hdbcli sqlalchemy-hana` or `pip install apache_superset[hana]`          | `hana://{username}:{password}@{host}:{port}`                                                                                                           |
| [StarRocks](/docs/configuration/databases#starrocks)                    | `pip install starrocks`                                                            | `starrocks://<User>:<Password>@<Host>:<Port>/<Catalog>.<Database>`                                                                                     |
| [Snowflake](/docs/configuration/databases#snowflake)                    | `pip install snowflake-sqlalchemy`                                                 | `snowflake://{user}:{password}@{account}.{region}/{database}?role={role}&warehouse={warehouse}`                                                        |
| SQLite                                                    | No additional library needed                                                       | `sqlite://path/to/file.db?check_same_thread=false`                                                                                                     |
| [SQL Server](/docs/configuration/databases#sql-server)                  | `pip install pymssql`                                                              | `mssql+pymssql://<Username>:<Password>@<Host>:<Port-default:1433>/<Database Name>`                                                                                                                                     |
| [TDengine](/docs/configuration/databases#tdengine)                      | `pip install taospy`  `pip install taos-ws-py`                                     | `taosws://<user>:<password>@<host>:<port>`                                                                                                             |
| [Teradata](/docs/configuration/databases#teradata)                      | `pip install teradatasqlalchemy`                                                   | `teradatasql://{user}:{password}@{host}`                                                                                                               |
| [TimescaleDB](/docs/configuration/databases#timescaledb)                | `pip install psycopg2`                                                             | `postgresql://<UserName>:<DBPassword>@<Database Host>:<Port>/<Database Name>`                                                                          |
| [Trino](/docs/configuration/databases#trino)                            | `pip install trino`                                                                | `trino://{username}:{password}@{hostname}:{port}/{catalog}`                                                                                            |
| [Vertica](/docs/configuration/databases#vertica)                        | `pip install sqlalchemy-vertica-python`                                            | `vertica+vertica_python://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                     |
| [YDB](/docs/configuration/databases#ydb)                                | `pip install ydb-sqlalchemy`                                                       | `ydb://{host}:{port}/{database_name}`                                                                                                                  |
| [YugabyteDB](/docs/configuration/databases#yugabytedb)                  | `pip install psycopg2`                                                             | `postgresql://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                                 |

---

他にも多くのデータベースがサポートされていることに注意してください。
主な基準は、機能的な SQLAlchemy 方言と Python ドライバーの存在です。
キーワード「sqlalchemy + (データベース名)」を検索すると、適切な場所にたどり着くことができます。

データベースまたはデータ エンジンがリストにないけれども、SQL インターフェイスが存在する場合は、[Superset GitHub リポジトリ](https://github.com/apache/superset/issues) に問題を報告してください。
そうすれば、ドキュメント化とサポートに取り組むことができます。

Superset 統合用のデータベース コネクタを構築する場合は、[次のチュートリアル](https://preset.io/blog/building-database-connector/) をお読みください。

### Docker イメージにドライバーをインストールする

Superset では、接続する追加のデータベースの種類ごとに Python データベース ドライバーをインストールする必要があります。

この例では、MySQL コネクタライブラリのインストール方法について説明します。
コネクタライブラリのインストールプロセスは、すべての追加ライブラリで同じです。

#### 1. 必要なドライバーを決定する

[データベースドライバ一覧](/docs/configuration/databases)を参照し、データベースへの接続に必要なPyPIパッケージを見つけてください。
この例ではMySQLデータベースに接続するため、`mysqlclient`コネクタライブラリが必要になります。

#### 2. コンテナにドライバーをインストールする

`mysqlclient` ライブラリを Superset Docker コンテナにインストールする必要があります (ホスト マシンにインストールされているかどうかは関係ありません)。
`docker exec -it <container_name> bash` を使用して実行中のコンテナに入り、そこで `pip install mysqlclient` を実行することもできますが、永続的に保持されるわけではありません。

この問題に対処するために、Superset `docker compose` デプロイメントでは `requirements-local.txt` ファイルの規則を使用します。
このファイルにリストされているすべてのパッケージは、実行時に PyPI からコンテナにインストールされます。
このファイルは、ローカル開発の目的で Git によって無視されます。

`docker-compose.yml` または `docker-compose-non-dev.yml` ファイルがあるディレクトリにある `docker` というサブディレクトリに `requirements-local.txt` ファイルを作成します。

```bash
# Run from the repo root:
touch ./docker/requirements-local.txt
```

上記の手順で特定したドライバーを追加します。
テキストエディタを使用するか、コマンドラインから次のように実行できます:

```bash
echo "mysqlclient" >> ./docker/requirements-local.txt
```

**標準の（カスタマイズされていない）Superset イメージを実行している場合**、これで完了です。
`docker compose -f docker-compose-non-dev.yml up` で Superset を起動すると、ドライバーがインストールされているはずです。

`docker exec -it <container_name> bash` で実行中のコンテナに入り、`pip freeze` を実行することで、ドライバーの存在を確認できます。
表示されるリストに PyPI パッケージが含まれているはずです。

**カスタマイズされた Docker イメージを実行している場合**、新しいドライバーを組み込んだローカルイメージを再構築します:

```bash
docker compose build --force-rm
```

Docker イメージの再構築が完了したら、`docker compose up` を実行して Superset を再起動します。

#### 3. MySQLに接続する

コンテナに MySQL ドライバーがインストールされたので、Superset の Web UI 経由でデータベースに接続できるようになります。

管理者ユーザーとして、「Settings」 -> 「Data: Database Connections」に移動し、「+DATABASE」ボタンをクリックします。
そこから、[データベース接続 UI の使用](/docs/configuration/databases#connecting-through-the-ui) の手順に従います。

Superset ドキュメントで、ご利用のデータベースの種類に応じたページを参照し、接続文字列とその他の入力パラメータを確認してください。
例えば、[MySQL ページ](/docs/configuration/databases#mysql) では、ローカル MySQL データベースへの接続文字列は、セットアップが Linux で実行されているか Mac で実行されているかによって異なります。

「Test Connection」 ボタンをクリックすると、「Connection looks good!」というポップアップメッセージが表示されます。

#### 4. トラブルシューティング

テストが失敗した場合は、Docker ログでエラー メッセージを確認してください。
Superset は SQLAlchemy を使用してデータベースに接続します。
データベースの接続文字列のトラブルシューティングを行うには、Superset アプリケーション コンテナーまたはホスト環境で Python を起動し、目的のデータベースに直接接続してデータを取得してみてください。
これにより、問題を切り分けるために Superset が排除されます。

Superset が接続するデータベースの種類ごとにこのプロセスを繰り返します。

### データベース固有の指示

#### Ascend.io

Ascend.io に推奨されるコネクタ ライブラリは [impyla](https://github.com/cloudera/impyla) です。

想定される接続文字列の形式は次のとおりです:

```
ascend://{username}:{password}@{hostname}:{port}/{database}?auth_mechanism=PLAIN;use_ssl=true
```

#### Apache Doris

SQLAlchemy 経由で Apache Doris に接続するには、[sqlalchemy-doris](https://pypi.org/project/pydoris/) ライブラリの使用が推奨されます。

接続文字列を作成するには、以下の設定値が必要です:

- **User**: User Name
- **Password**: Password
- **Host**: Doris FE Host
- **Port**: Doris FE port
- **Catalog**: Catalog Name
- **Database**: Database Name

接続文字列は次のようになります:

```
doris://<User>:<Password>@<Host>:<Port>/<Catalog>.<Database>
```

#### AWS Athena

##### PyAthenaJDBC

[PyAthenaJDBC](https://pypi.org/project/PyAthenaJDBC/) は、[Amazon Athena JDBC ドライバー](https://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html) 用の Python DB 2.0 準拠のラッパーです。

Amazon Athena の接続文字列は次のとおりです:

```
awsathena+jdbc://{aws_access_key_id}:{aws_secret_access_key}@athena.{region_name}.amazonaws.com/{schema_name}?s3_staging_dir={s3_staging_dir}&...
```

次のように接続文字列を形成するときは、エスケープとエンコードを行う必要があることに注意してください:

```
s3://... -> s3%3A//...
```

##### PyAthena

次の接続文字列を使用して、[PyAthena ライブラリ](https://pypi.org/project/PyAthena/) (Java は不要) を使用することもできます:

```
awsathena+rest://{aws_access_key_id}:{aws_secret_access_key}@athena.{region_name}.amazonaws.com/{schema_name}?s3_staging_dir={s3_staging_dir}&...
```

PyAthena ライブラリでは、Superset の Athena データベース接続 UI の [ADVANCED] --> [Other] --> [ENGINE PARAMETERS] で次のパラメータを追加することで定義できる特定の IAM ロールを引き受けることもできます。

```json
{
    "connect_args": {
        "role_arn": "<role arn>"
    }
}
```

#### AWS DynamoDB

##### PyDynamoDB

[PyDynamoDB](https://pypi.org/project/PyDynamoDB/) は、Amazon DynamoDB 用の Python DB API 2.0 (PEP 249) クライアントです。

Amazon DynamoDB への接続文字列は次のとおりです:

```
dynamodb://{aws_access_key_id}:{aws_secret_access_key}@dynamodb.{region_name}.amazonaws.com:443?connector=superset
```

詳細なドキュメントについては、[PyDynamoDB WIKI](https://github.com/passren/PyDynamoDB/wiki/5.-Superset) をご覧ください。

#### AWS Redshift

SQLAlchemy 経由で Redshift に接続するには、[sqlalchemy-redshift](https://pypi.org/project/sqlalchemy-redshift/) ライブラリの使用が推奨されます。

この方言が正しく動作するには、[redshift_connector](https://pypi.org/project/redshift-connector/) または [psycopg2](https://pypi.org/project/psycopg2/) のいずれかが必要です。

接続文字列を作成するには、以下の値を設定する必要があります:

- **User Name**: userName
- **Password**: DBPassword
- **Database Host**: AWS Endpoint
- **Database Name**: Database Name
- **Port**: default 5439

##### psycopg2

SQLALCHEMY URI は次のようになります:

```
redshift+psycopg2://<userName>:<DBPassword>@<AWS End Point>:5439/<Database Name>
```

##### redshift_connector

SQLALCHEMY URI は次のようになります:

```
redshift+redshift_connector://<userName>:<DBPassword>@<AWS End Point>:5439/<Database Name>
```

###### Redshift クラスターで IAM ベースの認証情報を使用する

[Amazon Redshift クラスター](https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html) は、一時的な IAM ベースのデータベースユーザー認証情報の生成もサポートしています。

superset アプリの [IAM ロールには、`redshift:GetClusterCredentials` オペレーションを呼び出すための権限が必要です](https://docs.aws.amazon.com/redshift/latest/mgmt/generating-iam-credentials-role-permissions.html)。

superset の Redshift データベース接続 UI の [ADVANCED] --> [Others] --> [ENGINE PARAMETERS] で、以下の引数を定義する必要があります。

```
{"connect_args":{"iam":true,"database":"<database>","cluster_identifier":"<cluster_identifier>","db_user":"<db_user>"}}
```

SQLALCHEMY URIは`redshift+redshift_connector://`に設定する必要があります。

###### Redshift サーバーレスで IAM ベースの認証情報を使用する

[Redshift Serverless](https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-whatis.html) は、IAM ロールを使用した接続をサポートしています。

Superset アプリの IAM ロールには、Redshift Serverless ワークグループに対する `redshift-serverless:GetCredentials` および `redshift-serverless:GetWorkgroup` 権限が必要です。

Superset の Redshift データベース接続 UI の「ADVANCED」->「Others」->「ENGINE PARAMETERS」で、以下の引数を定義する必要があります。

```
{"connect_args":{"iam":true,"is_serverless":true,"serverless_acct_id":"<aws account number>","serverless_work_group":"<redshift work group>","database":"<database>","user":"IAMR:<superset iam role name>"}}
```

#### ClickHouse

ClickHouse を Superset で使用するには、`clickhouse-connect` Python ライブラリをインストールする必要があります:

Docker Compose を使用して Superset を実行する場合は、`./docker/requirements-local.txt` ファイルに次の内容を追加します:

```
clickhouse-connect>=0.6.8
```

ClickHouse に推奨されるコネクタライブラリは [clickhouse-connect](https://github.com/ClickHouse/clickhouse-connect) です。

想定される接続文字列の形式は次のとおりです:

```
clickhousedb://<user>:<password>@<host>:<port>/<database>[?options…]clickhouse://{username}:{password}@{hostname}:{port}/{database}
```

実際の接続文字列の具体的な例を次に示します:

```
clickhousedb://demo:demo@github.demo.trial.altinity.cloud/default?secure=true
```

Clickhouse をコンピュータ上でローカルに使用している場合は、パスワードなしでデフォルトのユーザーを使用する (接続を暗号化しない) http プロトコル URL を使用すれば済みます:

```
clickhousedb://localhost/default
```

#### CockroachDB

CockroachDB に推奨されるコネクタライブラリは [sqlalchemy-cockroachdb](https://github.com/cockroachdb/sqlalchemy-cockroachdb) です。

想定される接続文字列の形式は次のとおりです:

```
cockroachdb://root@{hostname}:{port}/{database}?sslmode=disable
```

#### Couchbase

Couchbase の Superset 接続は、Couchbase Analytics と Couchbase Columnar の 2 つのサービスをサポートするように設計されています。
Couchbase に推奨されるコネクタライブラリは [couchbase-sqlalchemy](https://github.com/couchbase/couchbase-sqlalchemy) です。

```
pip install couchbase-sqlalchemy
```

予想される接続文字列の形式は次のとおりです:

```
couchbase://{username}:{password}@{hostname}:{port}?truststorepath={certificate path}?ssl={true/false}
```

#### CrateDB

CrateDB のコネクタライブラリは [sqlalchemy-cratedb] です。
`requirements.txt` ファイルに次の項目を追加することをお勧めします:

```
sqlalchemy-cratedb>=0.40.1,<1
```

評価目的でローカルホスト上の[CrateDB Self-Managed]のSQLAlchemy接続文字列は次のようになります:

```
crate://crate@127.0.0.1:4200
```

[CrateDB Cloud] に接続するための SQLAlchemy 接続文字列は次のようになります:

```
crate://<username>:<password>@<clustername>.cratedb.net:4200/?ssl=true
```

Docker Compose を使用してローカルで Superset を設定する場合は、[こちら](/docs/configuration/databases#installing-database-drivers) の手順に従って、CrateDB コネクタ パッケージをインストールしてください。

```
echo "sqlalchemy-cratedb" >> ./docker/requirements-local.txt
```

[CrateDB Cloud]: https://cratedb.com/product/cloud
[CrateDB Self-Managed]: https://cratedb.com/product/self-managed
[sqlalchemy-cratedb]: https://pypi.org/project/sqlalchemy-cratedb/

#### Databend

Databend の推奨コネクタライブラリは [databend-sqlalchemy](https://pypi.org/project/databend-sqlalchemy/) です。
Superset は `databend-sqlalchemy>=0.2.3` でテストされています。

推奨される接続文字列は次のとおりです。

```
databend://{username}:{password}@{host}:{port}/{database_name}
```

以下は、Superset が Databend データベースに接続する接続文字列の例です:

```
databend://user:password@localhost:8000/default?secure=false
```

#### Databricks

Databricks は現在、ネイティブ DB API 2.0 ドライバ「databricks-sql-connector」を提供しています。
これは「sqlalchemy-databricks」方言で使用できます。どちらも以下のコマンドでインストールできます:

```bash
pip install "apache-superset[databricks]"
```

Hive コネクタを使用するには、クラスターから次の情報を取得する必要があります:

- Server hostname
- Port
- HTTP path

これらは、「構成」→「詳細オプション」→「JDBC/ODBC」にあります。

また、「設定」→「ユーザー設定」→「アクセストークン」からアクセストークンを取得する必要があります。

これらの情報をすべて入手したら、「Databricks Native Connector」タイプのデータベースを追加し、以下のSQLAlchemy URIを使用します:

```
databricks+connector://token:{access_token}@{server_hostname}:{port}/{database_name}
```

また、HTTP パスを使用して、「その他」->「エンジン パラメータ」に次の構成を追加する必要があります:

```json
{
    "connect_args": {"http_path": "sql/protocolv1/o/****"}
}
```

##### Older driver

元々、SupersetはDatabricksへの接続に`databricks-dbapi`を使用していました。
公式のDatabricksコネクタで問題が発生している場合は、こちらを試してみると良いかもしれません:

```bash
pip install "databricks-dbapi[sqlalchemy]"
```

`databricks-dbapi` を使用する場合、Databricks に接続する方法は 2 つあります。
Hive コネクタを使用する方法と ODBC コネクタを使用する方法です。
どちらの方法も動作は似ていますが、[SQL エンドポイント](https://docs.databricks.com/sql/admin/sql-endpoints.html) への接続には ODBC のみを使用できます。

#### Hive

Hive クラスターに接続するには、Superset に「Databricks Interactive Cluster」タイプのデータベースを追加し、次の SQLAlchemy URI を使用します:

```
databricks+pyhive://token:{access_token}@{server_hostname}:{port}/{database_name}
```

また、HTTP パスを使用して、「Other」->「Engine Parameters」に次の構成を追加する必要があります。

```json
{"connect_args": {"http_path": "sql/protocolv1/o/****"}}
```

#### ODBC

ODBC を使用する場合は、まず [お使いのプラットフォーム用の ODBC ドライバー](https://databricks.com/spark/odbc-drivers-download) をインストールする必要があります。

通常の接続の場合は、ユースケースに応じてデータベースとして「Databricks Interactive Cluster」または「Databricks SQL Endpoint」を選択した後、これを SQLAlchemy URI として使用します:

```
databricks+pyodbc://token:{access_token}@{server_hostname}:{port}/{database_name}
```

接続引数については次のようになります:

```json
{"connect_args": {"http_path": "sql/protocolv1/o/****", "driver_path": "/path/to/odbc/driver"}}
```

ドライバーのパスは次のようになります:

- `/Library/simba/spark/lib/libsparkodbc_sbu.dylib` (Mac OS)
- `/opt/simba/spark/lib/64/libsparkodbc_sb64.so` (Linux)

SQL エンドポイントに接続するには、エンドポイントからの HTTP パスを使用する必要があります:

```json
{"connect_args": {"http_path": "/sql/1.0/endpoints/****", "driver_path": "/path/to/odbc/driver"}}
```

#### Denodo

Denodo に推奨されるコネクタライブラリは [denodo-sqlalchemy](https://pypi.org/project/denodo-sqlalchemy/) です。

想定される接続文字列の形式は次のとおりです（デフォルトポートは 9996 です）:

```
denodo://{username}:{password}@{hostname}:{port}/{database}
```

#### Dremio

Dremio に推奨されるコネクタライブラリは [sqlalchemy_dremio](https://pypi.org/project/sqlalchemy-dremio/) です。

ODBC の接続文字列（デフォルトポートは 31010）は、以下の形式になります:

```
dremio+pyodbc://{username}:{password}@{host}:{port}/{database_name}/dremio?SSL=1
```

Arrow Flight (Dremio 4.9.1+。デフォルト ポートは 32010) の予想される接続文字列は、次の形式になります:

```
dremio+flight://{username}:{password}@{host}:{port}/dremio
```

この [Dremio のブログ投稿](https://www.dremio.com/tutorials/dremio-apache-superset/) には、Superset を Dremio に接続するための追加の役立つ手順が記載されています。

#### Apache Drill

##### SQLAlchemy

Apache Drillへの接続には、SQLAlchemy経由の接続が推奨されます。
[sqlalchemy-drill](https://github.com/JohnOmernik/sqlalchemy-drill) パッケージを使用できます。

インストールが完了すると、RESTインターフェース経由またはJDBC経由の2つの方法でDrillに接続できるようになります。
JDBC経由で接続する場合は、Drill JDBCドライバーがインストールされている必要があります。

Drillの基本的な接続文字列は次のとおりです。

```
drill+sadrill://<username>:<password>@<host>:<port>/<storage_plugin>?use_ssl=True
```

組み込みモードで実行されているローカル マシンで実行されている Drill に接続するには、次の接続文字列を使用できます:

```
drill+sadrill://localhost:8047/dfs?use_ssl=False
```

##### JDBC

JDBC 経由で Drill に接続する方法はより複雑なため、[こちらのチュートリアル](https://drill.apache.org/docs/using-the-jdbc-driver/) に従うことをお勧めします。

接続文字列は次のようになります:

```
drill+jdbc://<username>:<password>@<host>:<port>
```

##### ODBC

ODBC を介して Drill を操作する方法を学ぶには、[Apache Drill のドキュメント](https://drill.apache.org/docs/installing-the-driver-on-linux/) を読み、[GitHub README](https://github.com/JohnOmernik/sqlalchemy-drill#usage-with-odbc) を読むことをお勧めします。

import useBaseUrl from "@docusaurus/useBaseUrl";

#### Apache Druid

Druid へのネイティブ コネクタは Superset (`DRUID_IS_ACTIVE` フラグの背後) に同梱されていますが、これは [pydruid ライブラリ](https://pythonhosted.org/pydruid/) で利用できる SQLAlchemy / DBAPI コネクタに取って代わられ、徐々に廃止されつつあります。

接続文字列は次のようになります:

```
druid://<User>:<password>@<Host>:<Port-default-9088>/druid/v2/sql
```

この接続文字列の主要な構成要素は次のとおりです。

- `User`: データベースへの接続に必要な認証情報のユーザー名部分
- `Password`: データベースへの接続に必要な認証情報のパスワード部分
- `Host`: データベースを実行しているホストマシンのIPアドレス（またはURL）
- `Port`: データベースを実行しているホストマシンで公開されている特定のポート

##### Druid 接続のカスタマイズ

Druid への接続を追加する際、**Add Database** フォームでいくつかの方法で接続をカスタマイズできます。

**カスタム証明書**

Druid への新しいデータベース接続を構成する際に、**Root Certificate** フィールドに証明書を追加できます:

<img src={useBaseUrl("/img/root-cert-example.png")} />{" "}

カスタム証明書を使用する場合、pydruid は自動的に https スキームを使用します。

**SSL 検証を無効にする**

SSL 検証を無効にするには、**Extras** フィールドに以下のコードを追加します:

```
engine_params:
{"connect_args":
 {"scheme": "https", "ssl_verify_cert": false}}
```

##### 集約

Superset では、一般的な集計や Druid メトリクスを定義・使用できます。最初の、そしてよりシンプルなユースケースは、データソースの編集ビューに表示されるチェックボックスマトリックスを使用することです (**Sources -> Druid Datasources -> [your datasource] -> Edit -> [tab] [List Druid Column]**)。

[GroupBy] および [Filterable] チェックボックスをオンにすると、Explore ビューで関連するドロップダウンに列が表示されます。
[Count Distinct]、[Min]、[Max]、または [Sum] をオンにすると、新しいメトリクスが作成され、データソースの保存時に **List Druid Metric** タブに表示されます。

これらのメトリクスを編集すると、その JSON 要素が Druid 集計定義に対応していることがわかります。
**List Druid Metric** タブから、Druid ドキュメントに従って独自の集計を手動で作成することもできます。

##### 集約後

Druidは集約後をサポートし、これはSuperSetで機能します。
あなたがしなければならないのは、あなたが手動で集約を作成するのと同じようにメトリックを作成することですが、「Postagg」を「メトリックタイプ」として指定します。
次に、JSONフィールドで有効なJSON後の凝集後定義（ドルイドドキュメントで指定）を提供する必要があります。

#### Elasticsearch

ElasticSearchの推奨コネクタライブラリは[Elasticsearch-dbapi]（https://github.com/preset-io/elasticsearch-dbapi）です。

ElasticSearchの接続文字列は次のようになります：

```
elasticsearch+http://{user}:{password}@{host}:9200/
```

**HTTPSを使用**

```
elasticsearch+https://{user}:{password}@{host}:9200/
```

ElasticSearchは10000行のデフォルト制限として、クラスターのこの制限を増やしたり、SuperSetの行の制限を設定したりすることができます。

```
ROW_LIMIT = 10000
```

たとえば、SQL Labで複数のインデックスをクエリすることができます

```
SELECT timestamp, agent FROM "logstash"
```

ただし、複数のインデックスに視覚化を使用するには、クラスターにエイリアスインデックスを作成する必要があります

```
POST /_aliases
{
    "actions" : [
        { "add" : { "index" : "logstash-**", "alias" : "logstash_all" } }
    ]
}
```

次に、エイリアス名logstash_allにテーブルを登録します

**タイムゾーン**

デフォルトでは、SuperSetはElasticSearchクエリにUTCタイムゾーンを使用します。
タイムゾーンを指定する必要がある場合は、データベースを編集し、指定されたタイムゾーンの設定を Other > ENGINE PARAMETERS に入力してください:

```json
{
    "connect_args": {
        "time_zone": "Asia/Shanghai"
    }
}
```

タイムゾーンの問題について注意すべきもう1つの問題は、Elasticsearch7.8の前に、文字列を「datetime」オブジェクトに変換する場合、「cast」関数を使用する必要があるが、この関数は「time_zone」設定をサポートしないことです。
したがって、ElasticSearch7.8の後にバージョンにアップグレードすることをお勧めします。
ElasticSearch7.8の後、「datetime_parse`関数を使用してこの問題を解決できます。
DateTime_Parse関数は、「time_zone」設定をサポートすることであり、ここでは、他の>バージョン設定でElasticSearchバージョン番号を入力する必要があります。
superset は、変換に「datetime_parse」関数を使用します。

**SSL検証を無効にします**

SSL検証を無効にするには、**SQLALCHEMY URI** フィールドに以下を追加します。

```
elasticsearch+https://{user}:{password}@{host}:9200/?verify_certs=False
```

#### Exasol

Exasolの推奨コネクタライブラリは[Sqlalchemy-exasol]（https://github.com/exasol/sqlalchemy-exasol）です。

Exasolの接続文字列は次のようになります：

```
exa+pyodbc://{username}:{password}@{hostname}:{port}/my_schema?CONNECTIONLCALL=en_US.UTF-8&driver=EXAODBC
```

#### Firebird

Firebirdの推奨コネクタライブラリは[sqlalchemy-firebird]（https://pypi.org/project/sqlalchemy-firebird/）です。
スーパーセットは、 `sqlalchemy-firebird> = 0.7.0、<0.8`でテストされています。

推奨される接続文字列は次のとおりです:

```
firebird+fdb://{username}:{password}@{host}:{port}//{path_to_db_file}
```

これは、ローカルファイアバードデータベースに接続する Superset の接続文字列の例です。

```
firebird+fdb://SYSDBA:masterkey@192.168.86.38:3050//Library/Frameworks/Firebird.framework/Versions/A/Resources/examples/empbuild/employee.fdb
```

#### Firebolt

Fireboltの推奨コネクタライブラリは[Firebolt-sqlalchemy]（https://pypi.org/project/firebolt-sqlalchemy/）です。

推奨される接続文字列は次のとおりです:

```
firebolt://{username}:{password}@{database}?account_name={name}
または
firebolt://{username}:{password}@{database}/{engine_name}?account_name={name}
```

サービスアカウントを使用して接続することも可能です:

```
firebolt://{client_id}:{client_secret}@{database}?account_name={name}
または
firebolt://{client_id}:{client_secret}@{database}/{engine_name}?account_name={name}
```

#### Google BigQuery

BigQueryの推奨コネクタライブラリは [sqlalchemy-bigquery](https://github.com/googleapis/python-bigquery-sqlalchemy) です。

##### BigQueryドライバーをインストールします

Docker Composeを介してSuperSetをローカルにセットアップするときに新しいデータベースドライバーをインストールする方法については、[こちら](/docs/configuration/databases#installing-drivers-in-docker-images) に従ってください。

```bash
echo "sqlalchemy-bigquery" >> ./docker/requirements-local.txt
```

##### BigQueryへの接続

SuperSetに新しいBigQuery接続を追加するときは、GCP Service Account Sredentialsファイル（JSONとして）を追加する必要があります。

1. Google Cloud Platformコントロールパネルを介してサービスアカウントを作成し、適切なBigQueryデータセットへのアクセスを提供し、サービスアカウントのJSON構成ファイルをダウンロードします。
2. Superset では、そのJSONをアップロードするか、次の形式でJSON Blobを追加することができます（これは資格情報JSONファイルのコンテンツになります）:

```json
{
        "type": "service_account",
        "project_id": "...",
        "private_key_id": "...",
        "private_key": "...",
        "client_email": "...",
        "client_id": "...",
        "auth_uri": "...",
        "token_uri": "...",
        "auth_provider_x509_cert_url": "...",
        "client_x509_cert_url": "..."
    }
```

![CleanShot 2021-10-22 at 04 18 11](https://user-images.githubusercontent.com/52086618/138352958-a18ef9cb-8880-4ef1-88c1-452a9f1b8105.gif)

3. さらに、代わりにSqlalchemy URIを介して接続できます

   BigQueryの接続文字列は次のように見えます：

   ```
   bigquery://{project_id}
   ```

   **Advanced**タブに移動し、次の形式でデータベース構成フォームの **Secure Extra** フィールドにJSONブロブを追加します。

   ```json
   {
   "credentials_info": <contents of credentials JSON file>
   }
   ```

   結果のファイルには、この構造が必要です:

   ```json
   {
    "credentials_info": {
        "type": "service_account",
        "project_id": "...",
        "private_key_id": "...",
        "private_key": "...",
        "client_email": "...",
        "client_id": "...",
        "auth_uri": "...",
        "token_uri": "...",
        "auth_provider_x509_cert_url": "...",
        "client_x509_cert_url": "..."
        }
    }
   ```

その後、BigQueryデータセットに接続できるはずです。

![CleanShot 2021-10-22 at 04 47 08](https://user-images.githubusercontent.com/52086618/138354340-df57f477-d3e5-42d4-b032-d901c69d2213.gif)

SuperSetのBigQueryにCSVまたはExcelファイルをアップロードできるようにするには、[pandas_gbq]（https://github.com/pydata/pandas-gbq）ライブラリを追加する必要があります。

現在、Google Bigquery Python SDKは、「Gevent」によるPython Coreライブラリでの動的なモンキーパッチがあるため、「Gevent」と互換性がありません。
したがって、「Gunicorn」サーバーでSuperSetを展開する場合、「Gevent」を除くワーカータイプを使用する必要があります。

#### Google Sheets

Googleシートには非常に限られた[SQL API](https://developers.google.com/chart/interactive/docs/querylanguage) があります。 Googleシート用の推奨コネクタライブラリは [Shillelagh](https://github.com/betodealmeida/shillelagh) です。

SuperSetをGoogleシートに接続することには、いくつかの手順があります。この[チュートリアル](https://preset.io/blog/2020-06-01-connect-superset-google-sheets/) には、この接続の設定に関する最新の指示があります。

#### Hana

推奨されるコネクタライブラリは[sqlalchemy-hana](https://github.com/sap/sqlalchemy-hana) です。

接続文字列は次のようにフォーマットされます:

```
hana://{username}:{password}@{host}:{port}
```

#### Apache Hive

[pyhive](https://pypi.org/project/pyhive/) ライブラリは、sqlalchemyを通じてハイブに接続する推奨方法です。

予想される接続文字列は、次のようにフォーマットされます:

```
hive://hive@{hostname}:{port}/{database}
```

#### Hologres

Hologres は、Alibaba Cloud が開発したリアルタイムのインタラクティブな分析サービスです。
PostgreSQL 11 と完全に互換性があり、BigData エコシステムとシームレスに統合します。

Hologres サンプル接続パラメーター：

- **User Name**: The AccessKey ID of your Alibaba Cloud account.
- **Password**: The AccessKey secret of your Alibaba Cloud account.
- **Database Host**: The public endpoint of the Hologres instance.
- **Database Name**: The name of the Hologres database.
- **Port**: The port number of the Hologres instance.

接続文字列は次のようになります:

```
postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}
```

#### IBM DB2

[IBM_DB_SA](https://github.com/ibmdb/python-ibmdbsa/master/ibm_db_sa) ライブラリは、IBM データサーバーに Python/SQLAlchemy インターフェイスを提供します。

これが推奨される接続文字列です：

```
db2+ibm_db://{username}:{passport}@{hostname}:{port}/{database}
```

Sqlalchemyに実装されている2つのDB2方言バージョンがあります。
`limit [n]` syntaxなしでdb2バージョンに接続している場合、SQLラボを使用できるように推奨される接続文字列は次のとおりです:

```
ibm_db_sa://{username}:{passport}@{hostname}:{port}/{database}
```

#### Apache Impala

Apache Impalaへの推奨コネクタライブラリは [Impyla](https://github.com/cloudera/impyla) です。

予想される接続文字列は、次のようにフォーマットされます:

```
impala://{hostname}:{port}/{database}
```

#### Kusto

Kustoの推奨コネクタライブラリは [sqlalchemy-kusto](https://pypi.org/project/sqlalchemy-kusto/2.0.0/) > = 2.0.0 です。

Kusto（sql dialect）の接続文字列は次のようになります:

```
kustosql+https://{cluster_url}/{database}?azure_ad_client_id={azure_ad_client_id}&azure_ad_client_secret={azure_ad_client_secret}&azure_ad_tenant_id={azure_ad_tenant_id}&msi=False
```

Kusto（kql dialect）の接続文字列は次のようになります。

```
kustokql+https://{cluster_url}/{database}?azure_ad_client_id={azure_ad_client_id}&azure_ad_client_secret={azure_ad_client_secret}&azure_ad_tenant_id={azure_ad_tenant_id}&msi=False
```

ユーザーがすべての必要なデータベース/テーブル/ビューにアクセスして使用する特権を持っていることを確認してください。

#### Apache Kylin

Apache Kylinの推奨コネクタライブラリは[Kylinpy]（https://github.com/kyligence/kylinpy）です。

予想される接続文字列は、次のようにフォーマットされます:

```
kylin://<username>:<password>@<hostname>:<port>/<project>?<param1>=<value1>&<param2>=<value2>
```

#### MySQL

mysqlの推奨コネクタライブラリは [mysqlclient](https://pypi.org/project/mysqlclient/) です。

これが接続文字列です:

```
mysql://{username}:{password}@{host}/{database}
```

Host:

- For Localhost: `localhost` or `127.0.0.1`
- Docker running on Linux: `172.18.0.1`
- For On Prem: IP address or Host name
- For Docker running in OSX: `docker.for.mac.host.internal`
  Port: `3306` by default

「mysqlclient」の問題の1つは、プラグインがクライアントに含まれていないため、認証用に「caching_sha2_password」を使用して新しいmysqlデータベースに接続できないことです。
この場合、[mysql-connector-python](https://pypi.org/project/mysql-connector-python/) を使用する必要があります。

```
mysql+mysqlconnector://{username}:{password}@{host}/{database}
```

#### IBM Netezza Performance Server

[nzalchemy](https://pypi.org/project/nzalchemy/) ライブラリは、IBM Netezza Performance Server（別名Netezza）にPython/SQLAlchemyインターフェイスを提供します。

これが推奨される接続文字列です:

```
netezza+nzpy://{username}:{password}@{hostname}:{port}/{database}
```

#### OceanBase

[sqlalchemy-oceanbase](https://pypi.org/project/oceanbase_py/) ライブラリは、SQLAlchemy を通じて OceanBase に接続する推奨方法です。

OceanBase の接続文字列は次のようになります：

```
oceanbase://<User>:<Password>@<Host>:<Port>/<Database>
```

#### Ocient DB

Ocientの推奨コネクタライブラリは[sqlalchemy-ocient]（https://pypi.org/project/sqlalchemy-ocient）です。

##### Ocient ドライバーのインストール

```bash
pip install sqlalchemy-ocient
```

##### COcient への接続

Ocient DSN の形式は次のとおりです:

```shell
ocient://user:password@[host][:port][/database][?param1=value1&...]
```

#### Oracle

推奨されるコネクタライブラリは [cx_oracle](https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html) です。

接続文字列は次のようにフォーマットされます。

```
oracle://<username>:<password>@<hostname>:<port>
```

#### Parseable

[Parsable](https://www.parsable.io) は、ログデータに SQL のようなクエリインターフェイスを提供する分散ログ分析データベースです。
推奨されるコネクタライブラリは [sqlalchemy-parsable](https://github.com/parsablehq/sqlalchemy-parsable) です。

接続文字列は次のようにフォーマットされます:

```
parseable://<username>:<password>@<hostname>:<port>/<stream_name>
```

例えば:

```
parseable://admin:admin@demo.parseable.com:443/ingress-nginx
```

注：URI の stream_name は、クエリする断続的なログストリームを表します。
HTTP（ポート80）とHTTPS（ポート443）接続の両方を使用できます。

>>>>>>>
#### Apache Pinot

Apache Pinotの推奨コネクタライブラリは[pinotdb](https://pypi.org/project/pinotdb/) です。

予想される接続文字列は、次のようにフォーマットされます:

```
pinot+http://<pinot-broker-host>:<pinot-broker-port>/query?controller=http://<pinot-controller-host>:<pinot-controller-port>/``
```

ユーザー名とパスワードを使用した予想される接続文字列は、次のようにフォーマットされます:

```
pinot://<username>:<password>@<pinot-broker-host>:<pinot-broker-port>/query/sql?controller=http://<pinot-controller-host>:<pinot-controller-port>/verify_ssl=true``
```

ビューや結合、ウィンドウ関数などをExploreで使用する場合は、[マルチステージクエリエンジン](https://docs.pinot.apache.org/reference/multi-stage-engine)を有効にします。
Advanced -> Other -> ENGINE PARAMETERS でデータベース接続を作成しながら、以下の引数を追加します

```json
{"connect_args":{"use_multistage_engine":"true"}}
```

#### Postgres

Docker Composeを使用している場合、Postgres コネクタライブラリ [psycopg2](https://www.psycopg.org/docs/) が Superset の箱から出てくることに注意してください。

Postgres sample connection parameters:

- **User Name**: UserName
- **Password**: DBPassword
- **Database Host**:
  - For Localhost: localhost or 127.0.0.1
  - For On Prem: IP address or Host name
  - For AWS Endpoint
- **Database Name**: Database Name
- **Port**: default 5432

接続文字列は次のようになります:

```
postgresql://{username}:{password}@{host}:{port}/{database}
```

最後に `？sslmode = require`を追加することで、SSLを要求できます:

```
postgresql://{username}:{password}@{host}:{port}/{database}?sslmode=require
```

[このドキュメントの表 31-1 ](https://www.postgresql.org/docs/9.1/libpq-ssl.html) でサポートする他の SSL モードについて読むことができます。

PostgreSQL接続オプションの詳細については、[SQLAlchemy のドキュメント](https://docs.sqlalchemy.org/en/13/dialects/postgresql.html#module-sqlalchemy.dialects.postgresql.postgg2) および [PostgreSQL のドキュメント](https://www.postgresql.org/docs/9.1/libpq-connect.html#LIBPQ-PQCONNECTDBPARAMS) を参照。

#### Presto

SQLAlchemy 経由で Presto に接続するには、[pyhive](https://pypi.org/project/PyHive/) ライブラリの使用が推奨されます。

想定される接続文字列の形式は次のとおりです:

```
presto://{hostname}:{port}/{database}
```

ユーザー名とパスワードも渡すことができます:

```
presto://{username}:{password}@{hostname}:{port}/{database}
```

値を含む接続文字列の例を次に示します:

```
presto://datascientist:securepassword@presto.example.com:8080/hive
```

Superset はデフォルトで、データソースのクエリ時に最新バージョンの Presto が使用されているものと想定します。古いバージョンの Presto を使用している場合は、extra パラメータで設定できます:

```json
{
    "version": "0.123"
}
```

SSL セキュア エクストラは、追加の接続情報に JSON 構成を追加します。

```json
   {
     "connect_args":
    {"protocol": "https",
     "requests_kwargs":{"verify":false}
  }
}
```

#### RisingWave

RisingWave に推奨されるコネクタライブラリは [sqlalchemy-risingwave](https://github.com/risingwavelabs/sqlalchemy-risingwave) です。

想定される接続文字列の形式は次のとおりです:

```
risingwave://root@{hostname}:{port}/{database}?sslmode=disable
```

#### Rockset

Rockset の接続文字列は次のとおりです:

```
rockset://{api key}:@{api server}
```

APIキーは[Rocksetコンソール](https://console.rockset.com/apikeys)から取得してください。
APIサーバーは[APIリファレンス](https://rockset.com/docs/rest-api/#introduction)から確認してください。URLの「https://」部分は省略してください。

特定の仮想インスタンスをターゲットにするには、次のURI形式を使用します。

```
rockset://{api key}:@{api server}/{VI ID}
```

より詳細な手順については、[Rockset のドキュメント](https://docs.rockset.com/apache-superset/) を参照することをお勧めします。

#### Snowflake

##### Snowflakeドライバーをインストールする

docker compose を使用してローカルで Superset を設定するときに新しいデータベース ドライバーをインストールする方法については、[こちら](/docs/configuration/databases#installing-database-drivers) の手順に従ってください。

```bash
echo "snowflake-sqlalchemy" >> ./docker/requirements-local.txt
```

Snowflake に推奨されるコネクタライブラリは [snowflake-sqlalchemy](https://pypi.org/project/snowflake-sqlalchemy/) です。

Snowflake の接続文字列は次のようになります:

```
snowflake://{user}:{password}@{account}.{region}/{database}?role={role}&warehouse={warehouse}
```

スキーマはテーブル/クエリごとに定義されるため、接続文字列には必要ありません。ロールとウェアハウスは、ユーザーにデフォルトが定義されている場合は省略できます。

```
snowflake://{user}:{password}@{account}.{region}/{database}
```

Snowflake SQLAlchemyエンジンは、デフォルトではエンジン作成時にユーザー/ロールの権限をテストしないため、必要なすべてのデータベース/スキーマ/テーブル/ビュー/ウェアハウスへのアクセス権限と使用権限がユーザーにあることを確認してください。
ただし、「データベースの作成」または「データベースの編集」ダイアログの「接続テスト」ボタンを押すと、エンジン作成時にconnect()メソッドに「validate_default_parameters」: Trueを渡すことで、ユーザー/ロールの資格情報が検証されます。
ユーザー/ロールにデータベースへのアクセス権限がない場合、Supersetログにエラーが記録されます。

また、Snowflake を [キーペア認証](https://docs.snowflake.com/en/user-guide/key-pair-auth.html#step-6-configure-the-snowflake-client-to-use-key-pair-authentication) で接続する場合は、キーペアがあること、および公開鍵が Snowflake に登録されていることを確認してください。
Snowflake をキーペア認証で接続するには、「SECURE EXTRA」フィールドに以下のパラメータを追加する必要があります。

***複数行に渡る秘密鍵の内容を 1 行にまとめ、各行の間に `\n` を挿入する必要があることに注意してください***

```json
{
     "auth_method": "keypair",
     "auth_params": {
         "privatekey_body": "-----BEGIN ENCRYPTED PRIVATE KEY-----\n...\n...\n-----END ENCRYPTED PRIVATE KEY-----",
         "privatekey_pass":"Your Private Key Password"
     }
 }
```

秘密鍵がサーバー上に保存されている場合は、パラメータ内の「privatekey_body」を「privatekey_path」に置き換えることができます。

```json
{
    "auth_method": "keypair",
    "auth_params": {
        "privatekey_path":"Your Private Key Path",
        "privatekey_pass":"Your Private Key Password"
    }
}
```

#### Apache Solr

[sqlalchemy-solr](https://pypi.org/project/sqlalchemy-solr/) ライブラリは、Apache Solr への Python / SQLAlchemy インターフェースを提供します。

Solr への接続文字列は次のようになります:

```
solr://{username}:{password}@{host}:{port}/{server_path}/{collection}[/?use_ssl=true|false]
```

#### Apache Spark SQL

Apache Spark SQL の推奨コネクタライブラリ [pyhive](https://pypi.org/project/PyHive/)。

想定される接続文字列の形式は次のとおりです:

```
hive://hive@{hostname}:{port}/{database}
```

#### SQL Server

SQL Server に推奨されるコネクタライブラリは [pymssql](https://github.com/pymssql/pymssql) です。

SQL Server の接続文字列は次のようになります:

```
mssql+pymssql://<Username>:<Password>@<Host>:<Port-default:1433>/<Database Name>
```

[pyodbc](https://pypi.org/project/pyodbc) にパラメータ [odbc_connect](https://docs.sqlalchemy.org/en/14/dialects/mssql.html#pass-through-exact-pyodbc-string) を指定して接続することも可能です。

SQL Server の接続文字列は次のようになります:

```
mssql+pyodbc:///?odbc_connect=Driver%3D%7BODBC+Driver+17+for+SQL+Server%7D%3BServer%3Dtcp%3A%3Cmy_server%3E%2C1433%3BDatabase%3Dmy_database%3BUid%3Dmy_user_name%3BPwd%3Dmy_password%3BEncrypt%3Dyes%3BConnection+Timeout%3D30
```

#### StarRocks

SQLAlchemy 経由で StarRocks に接続するには、[sqlalchemy-starrocks](https://pypi.org/project/starrocks/) ライブラリの使用が推奨されます。

接続文字列を作成するには、以下の設定値が必要です:

- **User**: User Name
- **Password**: DBPassword
- **Host**: StarRocks FE Host
- **Catalog**: Catalog Name
- **Database**: Database Name
- **Port**: StarRocks FE port

接続文字列は次のようになります:

```
starrocks://<User>:<Password>@<Host>:<Port>/<Catalog>.<Database>
```

:::note
StarRocks は、Superset のドキュメントを [こちら](https://docs.starrocks.io/docs/integrations/BI_integrations/Superset/) で管理しています。
:::

#### TDengine

[TDengine](https://www.tdengine.com) は、産業用 IoT 向けの高性能でスケーラブルな時系列データベースであり、SQL ライクなクエリインターフェースを提供します。

TDengine に推奨されるコネクタライブラリは、[taospy](https://pypi.org/project/taospy/) と [taos-ws-py](https://pypi.org/project/taos-ws-py/) です。

想定される接続文字列の形式は次のとおりです:

```
taosws://<user>:<password>@<host>:<port>
```

たとえば:

```
taosws://root:taosdata@127.0.0.1:6041
```

#### Teradata

推奨されるコネクタライブラリは [teradatasqlalchemy](https://pypi.org/project/teradatasqlalchemy/) です。

Teradata の接続文字列は次のようになります:

```
teradatasql://{user}:{password}@{host}
```

#### ODBC Driver

[sqlalchemy-teradata](https://github.com/Teradata/sqlalchemy-teradata) という古いコネクタもありますが、ODBC ドライバーのインストールが必要です。Teradata ODBC ドライバーは https://downloads.teradata.com/download/connectivity/odbc-driver/linux から入手できます。

必要な環境変数は次のとおりです:

```bash
export ODBCINI=/.../teradata/client/ODBC_64/odbc.ini
export ODBCINST=/.../teradata/client/ODBC_64/odbcinst.ini
```

ODBC ドライバーに関する要件が少なく、より定期的に更新されるため、最初のライブラリを使用することをお勧めします。

#### TimescaleDB

[TimescaleDB](https://www.timescale.com) は、時系列データや分析のためのオープンソースのリレーショナルデータベースであり、強力なデータ集約型アプリケーションを構築できます。
TimescaleDB は PostgreSQL の拡張機能であり、標準の PostgreSQL コネクタライブラリである [psycopg2](https://www.psycopg.org/docs/) を使用してデータベースに接続できます。

docker compose を使用している場合、Superset には psycopg2 が標準で付属しています。

TimescaleDB のサンプル接続パラメータ:

- **User Name**: User
- **Password**: Password
- **Database Host**:
  - For Localhost: localhost or 127.0.0.1
  - For On Prem: IP address or Host name
  - For [Timescale Cloud](https://console.cloud.timescale.com) service: Host name
  - For [Managed Service for TimescaleDB](https://portal.managed.timescale.com) service: Host name
- **Database Name**: Database Name
- **Port**: default 5432 or Port number of the service

接続文字列は次のようになります:

```
postgresql://{username}:{password}@{host}:{port}/{database name}
```

最後に `?sslmode=require` を追加することで SSL を必須にすることができます (例: [Timescale Cloud](https://www.timescale.com/cloud) を使用する場合):

```
postgresql://{username}:{password}@{host}:{port}/{database name}?sslmode=require
```

[TimescaleDB について詳しくはこちら!](https://docs.timescale.com/)

#### Trino

trinoバージョン352以上をサポート

##### Connection String

接続文字列の形式は次のとおりです:

```
trino://{username}:{password}@{hostname}:{port}/{catalog}
```

ローカルマシンでdockerを使ってTrinoを実行している場合は、次の接続URLを使用してください。

```
trino://trino@host.docker.internal:8080
```

##### Authentications

###### 1. Basic Authentication

接続文字列または「Advanced / Security」の「Secure Extra」フィールドに「ユーザー名」/「パスワード」を入力できます。

- 接続文字列では

    ```
    trino://{username}:{password}@{hostname}:{port}/{catalog}
    ```

- `Secure Extra` フィールドでは

    ```json
    {
        "auth_method": "basic",
        "auth_params": {
            "username": "<username>",
            "password": "<password>"
        }
    }
    ```

注意: 両方が指定されている場合は、`Secure Extra` が常に優先されます。

###### 2. Kerberos Authentication

「Secure Extra」フィールドで、次の例のように設定します:

```json
{
    "auth_method": "kerberos",
    "auth_params": {
        "service_name": "superset",
        "config": "/path/to/krb5.config",
        ...
    }
}
```

`auth_params` 内のすべてのフィールドは、[`KerberosAuthentication`](https://github.com/trinodb/trino-python-client/blob/0.306.0/trino/auth.py#L40) クラスに直接渡されます。

注: Kerberos 認証を使用するには、[`trino-python-client`](https://github.com/trinodb/trino-python-client) を `all` または `kerberos` オプション機能付きでローカルにインストールする必要があります。つまり、それぞれ `trino[all]` または `trino[kerberos]` をインストールする必要があります。

###### 3. Certificate Authentication

「Secure Extra」フィールドで、次の例のように設定します:

```json
{
    "auth_method": "certificate",
    "auth_params": {
        "cert": "/path/to/cert.pem",
        "key": "/path/to/key.pem"
    }
}
```

`auth_params` 内のすべてのフィールドは [`CertificateAuthentication`](https://github.com/trinodb/trino-python-client/blob/0.315.0/trino/auth.py#L416) クラスに直接渡されます。

###### 4. JWT Authentication

`auth_method` を設定し、`Secure Extra` フィールドにトークンを入力します。

```json
{
    "auth_method": "jwt",
    "auth_params": {
        "token": "<your-jwt-token>"
    }
}
```

###### 5. Custom Authentication

カスタム認証を使用するには、まず、Superset 構成ファイルの `ALLOWED_EXTRA_AUTHENTICATIONS` 許可リストにそれを追加する必要があります:

```python
from your.module import AuthClass
from another.extra import auth_method

ALLOWED_EXTRA_AUTHENTICATIONS: Dict[str, Dict[str, Callable[..., Any]]] = {
    "trino": {
        "custom_auth": AuthClass,
        "another_auth_method": auth_method,
    },
}
```

次に、「Secure Extra」フィールドで次の操作を行います:

```json
{
    "auth_method": "custom_auth",
    "auth_params": {
        ...
    }
}
```

`trino.auth.Authentication` クラスまたはファクトリー関数（`Authentication` インスタンスを返す）への参照を `auth_method` に渡すことで、カスタム認証を使用することもできます。

`auth_params` 内のすべてのフィールドは、クラス/関数に直接渡されます。

**参考**:

- [Trino-Superset-Podcast](https://trino.io/episodes/12.html)

#### Vertica

推奨されるコネクタライブラリは[sqlalchemy-vertica-python](https://pypi.org/project/sqlalchemy-vertica-python/)です。
[Vertica](http://www.vertica.com/)の接続パラメータは次のとおりです:

- **User Name:** UserName
- **Password:** DBPassword
- **Database Host:**
  - For Localhost : localhost or 127.0.0.1
  - For On Prem : IP address or Host name
  - For Cloud: IP Address or Host Name
- **Database Name:** Database Name
- **Port:** default 5433

接続文字列の形式は次のとおりです:

```
vertica+vertica_python://{username}:{password}@{host}/{database}
```

その他のパラメータ:

- Load Balancer - Backup Host

#### YDB

[YDB](https://ydb.tech/) に推奨されるコネクタ ライブラリは [ydb-sqlalchemy](https://pypi.org/project/ydb-sqlalchemy/) です。

##### Connection String

YDB の接続文字列は次のようになります:

```
ydb://{host}:{port}/{database_name}
```

##### Protocol

`Advanced / Security` の `Secure Extra` フィールドで `protocol` を指定できます:

```
{
    "protocol": "grpcs"
}
```

デフォルトは `grpc` です。

##### Authentication Methods

###### Static Credentials

`Static Credentials` を使用するには、`Advanced / Security` の `Secure Extra`フィールドに `username`/`password` を入力する必要があります:

```
{
    "credentials": {
        "username": "...",
        "password": "..."
    }
}
```

###### Access Token Credentials

`Access Token Credentials` を使用するには、`Advanced / Security` の `Secure Extra` フィールドに `token` を入力する必要があります:

```
{
    "credentials": {
        "token": "...",
    }
}
```

##### Service Account Credentials

サービス アカウント認証情報を使用するには、`Advanced / Security` の `Secure Extra` フィールドに `service_account_json` を指定する必要があります:

```
{
    "credentials": {
        "service_account_json": {
            "id": "...",
            "service_account_id": "...",
            "created_at": "...",
            "key_algorithm": "...",
            "public_key": "...",
            "private_key": "..."
        }
    }
}
```

#### YugabyteDB

[YugabyteDB](https://www.yugabyte.com/) は、PostgreSQL 上に構築された分散 SQL データベースです。

docker compose を使用している場合は、Superset に Postgres コネクタライブラリ [psycopg2](https://www.psycopg.org/docs/) が標準で付属していることに注意してください。

接続文字列は次のようになります:

```
postgresql://{username}:{password}@{host}:{port}/{database}
```

## UI 経由で接続する

新しいDB接続UIを活用する方法については、こちらのドキュメントをご覧ください。これにより、管理者は新しいデータベースに接続したいユーザーのUXを向上させることができます。

![db-conn-docs](https://user-images.githubusercontent.com/27827808/125499607-94e300aa-1c0f-4c60-b199-3f9de41060a3.gif)

新しい UI では、データベースへの接続に 3 つのステップが設けられています。

ステップ 1: まず、管理者は superset に対して、接続するエンジンを指定する必要があります。このページは `/available` エンドポイントに基づいており、現在環境にインストールされているエンジンを参照することで、サポートされているデータベースのみが表示されます。

ステップ 2: 次に、管理者はデータベース固有のパラメータを入力するよう求められます。特定のエンジンで利用可能な動的フォームがあるかどうかに応じて、管理者には新しいカスタムフォームまたは従来の SQLAlchemy フォームが表示されます。現在、Redshift、MySQL、Postgres、BigQuery 用の動的フォームを構築済みです。新しいフォームでは、接続に必要なパラメータ（ユーザー名、パスワード、ホスト、ポートなど）の入力が求められ、エラーが発生した場合は即座にフィードバックが提供されます。

ステップ 3: 最後に、管理者が動的フォームを使用してデータベースに接続したら、必要に応じて詳細設定を更新できます。

この機能により、ユーザーがアプリケーションにアクセスしてデータセットの作成を開始する際の大きなボトルネックが解消されることを期待しています。

##### 優先データベースオプションと画像の設定方法

管理者が優先データベースを次の順序で定義できる新しい構成オプションを追加しました:

```python
# A list of preferred databases, in order. These databases will be
# displayed prominently in the "Add Database" dialog. You should
# use the "engine_name" attribute of the corresponding DB engine spec
# in `superset/db_engine_specs/`.
PREFERRED_DATABASES: list[str] = [
    "PostgreSQL",
    "Presto",
    "MySQL",
    "SQLite",
]
```

著作権上の理由により、各データベースのロゴは Superset では配布されません。

##### 画像の設定

- 優先データベースのイメージを設定するには、管理者は`superset_text.yml`ファイルにエンジンとイメージの場所のマッピングを作成する必要があります。イメージは、static/fileディレクトリ内のローカルホスト、またはオンライン（例：S3）にホストできます。

```python
DB_IMAGES:
  postgresql: "path/to/image/postgres.jpg"
  bigquery: "path/to/s3bucket/bigquery.jpg"
  snowflake: "path/to/image/snowflake.jpg"
```

##### 利用可能なエンドポイントに新しいデータベースエンジンを追加する方法

現在、新しいモーダルは以下のデータベースをサポートしています:

- Postgres
- Redshift
- MySQL
- BigQuery

ユーザーがこのリストにないデータベースを選択した場合、SQLAlchemy URIの入力を求める従来のダイアログが表示されます。新しいデータベースは、新しいフローに段階的に追加できます。豊富な設定をサポートするには、DBエンジン仕様に以下の属性が必要です。

1. `parameters_schema`: データベースの設定に必要なパラメータを定義する Marshmallow スキーマ。Postgres の場合、ユーザー名、パスワード、ホスト、ポートなどが含まれます ([参照](https://github.com/apache/superset/blob/accee507c0819cd0d7bcfb5a3e1199bc81eeebf2/superset/db_engine_specs/base.py#L1309-L1320))。
2. `default_driver`: DB エンジン仕様で推奨されるドライバ名。多くの SQLAlchemy 方言は複数のドライバをサポートしていますが、通常はそのうちの 1 つが公式推奨です。Postgres の場合は "psycopg2" を使用します。
3. `sqlalchemy_uri_placeholder`: ユーザーが URI を直接入力する場合に便利な文字列。
4. `encryption_parameters`: ユーザーが暗号化接続を選択した場合にURIを構築するために使用するパラメータ。Postgresの場合、これは `{"sslmode": "require"}` です。

さらに、DBエンジン仕様では以下のクラスメソッドを実装する必要があります。

- `build_sqlalchemy_uri(cls, parameters, encrypted_extra)`: このメソッドは個別のパラメータを受け取り、それらからURIを構築します。
- `get_parameters_from_uri(cls, uri, encrypted_extra)`: このメソッドは、指定されたURIからパラメータを抽出します。
- `validate_parameters(cls, parameters)`: このメソッドは、フォームの`onBlur`検証に使用されます。不足しているパラメータと、明らかに間違っているパラメータを示す`SupersetError`のリストを返す必要があります（[例](https://github.com/apache/superset/blob/accee507c0819cd0d7bcfb5a3e1199bc81eeebf2/superset/db_engine_specs/base.py#L1404)）。

MySQLやPostgresなど、`engine+driver://user:password@host:port/dbname`という標準フォーマットを使用するデータベースの場合、DBエンジン仕様に`BasicParametersMixin`を追加し、パラメータ2～4を定義するだけです（`parameters_schema`は既にMixinに含まれています）。

その他のデータベースの場合は、これらのメソッドを自分で実装する必要があります。BigQuery DBエンジン仕様は、その方法の良い例です。

### 追加のデータベース設定

##### より深い SQLAlchemy 統合

SQLAlchemy が公開するパラメータを使用して、データベース接続情報を微調整することができます。
**Database edit** ビューでは、**Extra** フィールドを JSON BLOB として編集できます。

この JSON 文字列には追加の構成要素が含まれています。`engine_params` オブジェクトは `sqlalchemy.create_engine` 呼び出しに展開され、`metadata_params` は `sqlalchemy.MetaData` 呼び出しに展開されます。詳細については、SQLAlchemy のドキュメントを参照してください。

##### スキーマ

PostgresやRedshiftなどのデータベースは、**データベース**の上位にある論理エンティティとして**スキーマ**を使用します。
Supersetが特定のスキーマに接続するには、**Edit Tables** フォーム（[ソース] > [テーブル] > [レコードの編集]）で **schema** パラメータを設定できます。

##### SQLAlchemy 接続用の外部パスワードストア

Superset は、データベースパスワードを外部ストアに保存するように設定できます。
これは、カスタムのシークレット配布フレームワークを実行していて、Superset のメタデータにシークレットを保存したくない場合に便利です。

例: `sqla.engine.url` 型の引数を1つ受け取り、指定された接続文字列のパスワードを返す関数を作成します。
次に、設定ファイルで `SQLALCHEMY_CUSTOM_PASSWORD_STORE` を設定して、その関数を指定します。

```python
def example_lookup_password(url):
    secret = <<get password from external framework>>
    return 'secret'

SQLALCHEMY_CUSTOM_PASSWORD_STORE = example_lookup_password
```

一般的なパターンは、環境変数を使用してシークレットを利用できるようにすることです。
`SQLALCHEMY_CUSTOM_PASSWORD_STORE` もこの目的に使用できます。

```python
def example_password_as_env_var(url):
    # assuming the uri looks like
    # mysql://localhost?superset_user:{SUPERSET_PASSWORD}
    return url.password.format(**os.environ)

SQLALCHEMY_CUSTOM_PASSWORD_STORE = example_password_as_env_var
```

##### データベースへの SSL アクセス

**Edit Databases** フォームの `Extra` フィールドを使用して SSL を設定できます。

```JSON
{
    "metadata_params": {},
    "engine_params": {
          "connect_args":{
              "sslmode":"require",
              "sslrootcert": "/path/to/my/pem"
        }
     }
}
```

## その他

### データベース間のクエリ

Superset は、異なるデータベース間でクエリを実行するための実験的な機能を提供します。
これは、「superset://」SQLAlchemy URI を使用する「Superset メタデータベース」と呼ばれる特別なデータベースを介して実行されます。
このデータベースを使用すると、次の構文を使用して、設定されたデータベース内の任意のテーブルに対してクエリを実行できます:

```sql
SELECT * FROM "database name.[[catalog.].schema].table name";
```

たとえば:

```sql
SELECT * FROM "examples.birth_names";
```

スペースは使用できますが、名前内のピリオドは `%2E` に置き換える必要があります。例:

```sql
SELECT * FROM "Superset meta database.examples%2Ebirth_names";
```

上記のクエリは、`SELECT * FROM "examples.birth_names"` と同じ行を返します。また、メタ データベースが任意のテーブル (さらにはメタ データベース自体) からテーブルをクエリできることも示しています。

#### 考慮事項

この機能を有効にする前に、いくつか考慮すべき点があります。
まず、メタデータベースはクエリ対象のテーブルに対して権限を適用するため、ユーザーはデータベース経由でアクセスできるのは、元々アクセス権を持っているテーブルのみとなります。
しかしながら、メタデータベースは潜在的な攻撃の新たな標的となり、バグによってユーザーが本来閲覧すべきでないデータを閲覧できてしまう可能性があります。

次に、パフォーマンスに関する考慮事項があります。
メタデータベースは、フィルタリング、並べ替え、制限を基盤となるデータベースにプッシュしますが、集計と結合はクエリ実行プロセス中のメモリ内で行われます。
そのため、データベースを非同期モードで実行し、クエリがWebワーカーではなくCeleryワーカーで実行されるようにすることをお勧めします。
さらに、基盤となるデータベースから返される行数にハードリミットを指定することも可能です。

#### メタデータベースの有効化

Superset メタデータデータベースを有効にするには、まず `ENABLE_SUPERSET_META_DB` 機能フラグを true に設定する必要があります。
次に、SQLAlchemy URI「superset://」を使用して、「Superset meta database」タイプの新しいデータベースを追加します。

メタデータデータベースで DML を有効にすると、**基盤となるデータベースでも DML が有効になっている限り**、ユーザーは DML クエリを実行できるようになります。
これにより、ユーザーはデータベース間でデータを移動するクエリを実行できるようになります。

次に、`SUPERSET_META_DB_LIMIT` の値を変更することをおすすめします。
デフォルト値は 1000 で、集計や結合を実行する前に各データベースから読み取るデータ数を定義します。
小さなテーブルしかない場合は、この値を `None` に設定することもできます。

さらに、メタデータデータベースがアクセスできるデータベースを制限することもできます。これは、データベース構成の「Advanced」->「Other」->「ENGINE PARAMETERS」で、次の項目を追加することで実行できます。

```json
{"allowed_dbs":["Google Sheets","examples"]}
```
