---
title: データベースへの接続
hide_title: true
sidebar_position: 1
version: 1
---
# データベースへの接続

Superset には、データベースへの接続機能がバンドルされていません。Superset をデータベースに接続するための主な手順は、**適切なデータベース ドライバーを環境にインストールする** ことです。

:::note
メタデータ データベースとして使用するデータベースに必要なパッケージと、Superset 経由でアクセスするデータベースに接続するために必要なパッケージをインストールする必要があります。
Superset のメタデータ データベースの設定については、インストール ドキュメント ([Docker Compose](/docs/installation/docker-compose)、[Kubernetes](/docs/installation/kubernetes)) を参照してください。
:::

このドキュメントでは、一般的に使用されるデータベース エンジンのさまざまなドライバーへのポインターを維持しようとします。

## データベースドライバーのインストール

Superset では、接続するデータベース エンジンごとに、Python [DB-API データベース ドライバー](https://peps.python.org/pep-0249/) と [SQLAlchemy 方言](https://docs.sqlalchemy.org/en/20/dialects/) がインストールされている必要があります。

Superset 構成に新しいデータベース ドライバーをインストールする方法については、[こちら](/docs/configuration/databases#installing-drivers-in-docker-images) で詳しく読むことができます。

### サポートされているデータベースと依存関係

推奨パッケージの一部を以下に示します。Supersetと互換性のあるバージョンについては、[pyproject.toml](https://github.com/apache/superset/blob/master/pyproject.toml) を参照してください。

| <div style={{width: '150px'}}>Database</div>                   | PyPI package                                                                       | Connection String                                                                                                                                      |
| --------------------------------------------------------- | ---------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [AWS Athena](/docs/configuration/databases#aws-athena)                   | `pip install pyathena[pandas]` , `pip install PyAthenaJDBC`                        | `awsathena+rest://{access_key_id}:{access_key}@athena.{region}.amazonaws.com/{schema}?s3_staging_dir={s3_staging_dir}&...`   |
| [AWS DynamoDB](/docs/configuration/databases#aws-dynamodb)               | `pip install pydynamodb`                                                           | `dynamodb://{access_key_id}:{secret_access_key}@dynamodb.{region_name}.amazonaws.com?connector=superset`                                               |
| [AWS Redshift](/docs/configuration/databases#aws-redshift)               | `pip install sqlalchemy-redshift`                                                  | `redshift+psycopg2://<userName>:<DBPassword>@<AWS End Point>:5439/<Database Name>`                                                                    |
| [Apache Doris](/docs/configuration/databases#apache-doris)                     | `pip install pydoris`                                                              | `doris://<User>:<Password>@<Host>:<Port>/<Catalog>.<Database>`                                            |
| [Apache Drill](/docs/configuration/databases#apache-drill)                     | `pip install sqlalchemy-drill`                                                     | `drill+sadrill://<username>:<password>@<host>:<port>/<storage_plugin>`, often useful: `?use_ssl=True/False`                                                                                                              |
| [Apache Druid](/docs/configuration/databases#apache-druid)                     | `pip install pydruid`                                                              | `druid://<User>:<password>@<Host>:<Port-default-9088>/druid/v2/sql`                                                                                    |
| [Apache Hive](/docs/configuration/databases#hive)                       | `pip install pyhive`                                                               | `hive://hive@{hostname}:{port}/{database}`                                                                                                             |
| [Apache Impala](/docs/configuration/databases#apache-impala)                   | `pip install impyla`                                                               | `impala://{hostname}:{port}/{database}`                                                                                                                |
| [Apache Kylin](/docs/configuration/databases#apache-kylin)                     | `pip install kylinpy`                                                              | `kylin://<username>:<password>@<hostname>:<port>/<project>?<param1>=<value1>&<param2>=<value2>`                                                        |
| [Apache Pinot](/docs/configuration/databases#apache-pinot)                     | `pip install pinotdb`                                                              | `pinot://BROKER:5436/query?server=http://CONTROLLER:5983/`                                                                                             |
| [Apache Solr](/docs/configuration/databases#apache-solr)                       | `pip install sqlalchemy-solr`                                                      | `solr://{username}:{password}@{hostname}:{port}/{server_path}/{collection}`                                                                            |
| [Apache Spark SQL](/docs/configuration/databases#apache-spark-sql)             | `pip install pyhive`                                                               | `hive://hive@{hostname}:{port}/{database}`                                                                                                             |
| [Ascend.io](/docs/configuration/databases#ascendio)                       | `pip install impyla`                                                               | `ascend://{username}:{password}@{hostname}:{port}/{database}?auth_mechanism=PLAIN;use_ssl=true`                                                        |
| [Azure MS SQL](/docs/configuration/databases#sql-server)                | `pip install pymssql`                                                              | `mssql+pymssql://UserName@presetSQL:TestPassword@presetSQL.database.windows.net:1433/TestSchema`                                                       |
| [ClickHouse](/docs/configuration/databases#clickhouse)                  | `pip install clickhouse-connect`                                                   | `clickhousedb://{username}:{password}@{hostname}:{port}/{database}`                                                                                    |
| [CockroachDB](/docs/configuration/databases#cockroachdb)                | `pip install cockroachdb`                                                          | `cockroachdb://root@{hostname}:{port}/{database}?sslmode=disable`                                                                                      |
| [Couchbase](/docs/configuration/databases#couchbase)                | `pip install couchbase-sqlalchemy`                                                          | `couchbase://{username}:{password}@{hostname}:{port}?truststorepath={ssl certificate path}`                                                                                      |
| [CrateDB](/docs/configuration/databases#cratedb)                        | `pip install sqlalchemy-cratedb`                                                   | `crate://{username}:{password}@{hostname}:{port}`, often useful: `?ssl=true/false` or `?schema=testdrive`.                                             |
| [Denodo](/docs/configuration/databases#denodo)                          | `pip install denodo-sqlalchemy`                                                    | `denodo://{username}:{password}@{hostname}:{port}/{database}`                                                                                          |
| [Dremio](/docs/configuration/databases#dremio)                          | `pip install sqlalchemy_dremio`                                                    |`dremio+flight://{username}:{password}@{host}:32010`, often useful: `?UseEncryption=true/false`. For Legacy ODBC: `dremio+pyodbc://{username}:{password}@{host}:31010`                                                                                                                           |
| [Elasticsearch](/docs/configuration/databases#elasticsearch)            | `pip install elasticsearch-dbapi`                                                  | `elasticsearch+http://{user}:{password}@{host}:9200/`                                                                                                  |
| [Exasol](/docs/configuration/databases#exasol)                          | `pip install sqlalchemy-exasol`                                                    | `exa+pyodbc://{username}:{password}@{hostname}:{port}/my_schema?CONNECTIONLCALL=en_US.UTF-8&driver=EXAODBC`                                            |
| [Google BigQuery](/docs/configuration/databases#google-bigquery)                     | `pip install sqlalchemy-bigquery`                                                  | `bigquery://{project_id}`                                                                                                                              |
| [Google Sheets](/docs/configuration/databases#google-sheets)            | `pip install shillelagh[gsheetsapi]`                                               | `gsheets://`                                                                                                                                           |
| [Firebolt](/docs/configuration/databases#firebolt)                      | `pip install firebolt-sqlalchemy`                                                  | `firebolt://{client_id}:{client_secret}@{database}/{engine_name}?account_name={name}`                                                                  |
| [Hologres](/docs/configuration/databases#hologres)                      | `pip install psycopg2`                                                             | `postgresql+psycopg2://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                        |
| [IBM Db2](/docs/configuration/databases#ibm-db2)                        | `pip install ibm_db_sa`                                                            | `db2+ibm_db://`                                                                                                                                        |
| [IBM Netezza Performance Server](/docs/configuration/databases#ibm-netezza-performance-server) | `pip install nzalchemy`                                                            | `netezza+nzpy://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                               |
| [MySQL](/docs/configuration/databases#mysql)                            | `pip install mysqlclient`                                                          | `mysql://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                                      |
| [OceanBase](/docs/configuration/databases#oceanbase)                    | `pip install oceanbase_py`                                                         | `oceanbase://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                                      |
| [Oracle](/docs/configuration/databases#oracle)                          | `pip install cx_Oracle`                                                            | `oracle://<username>:<password>@<hostname>:<port>`                                                                                                                                            |
| [Parseable](/docs/configuration/databases#parseable)                    | `pip install sqlalchemy-parseable`                                                 | `parseable://<UserName>:<DBPassword>@<Database Host>/<Stream Name>`                                                                                    |
| [PostgreSQL](/docs/configuration/databases#postgres)                    | `pip install psycopg2`                                                             | `postgresql://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                                 |
| [Presto](/docs/configuration/databases#presto)                          | `pip install pyhive`                                                               | `presto://{username}:{password}@{hostname}:{port}/{database}`                                                                                                                                            |
| [Rockset](/docs/configuration/databases#rockset)                        | `pip install rockset-sqlalchemy`                                                   | `rockset://<api_key>:@<api_server>`                                                                                                                    |
| [SAP Hana](/docs/configuration/databases#hana)                          | `pip install hdbcli sqlalchemy-hana` or `pip install apache_superset[hana]`          | `hana://{username}:{password}@{host}:{port}`                                                                                                           |
| [StarRocks](/docs/configuration/databases#starrocks)                    | `pip install starrocks`                                                            | `starrocks://<User>:<Password>@<Host>:<Port>/<Catalog>.<Database>`                                                                                     |
| [Snowflake](/docs/configuration/databases#snowflake)                    | `pip install snowflake-sqlalchemy`                                                 | `snowflake://{user}:{password}@{account}.{region}/{database}?role={role}&warehouse={warehouse}`                                                        |
| SQLite                                                    | No additional library needed                                                       | `sqlite://path/to/file.db?check_same_thread=false`                                                                                                     |
| [SQL Server](/docs/configuration/databases#sql-server)                  | `pip install pymssql`                                                              | `mssql+pymssql://<Username>:<Password>@<Host>:<Port-default:1433>/<Database Name>`                                                                                                                                     |
| [TDengine](/docs/configuration/databases#tdengine)                      | `pip install taospy`  `pip install taos-ws-py`                                     | `taosws://<user>:<password>@<host>:<port>`                                                                                                             |
| [Teradata](/docs/configuration/databases#teradata)                      | `pip install teradatasqlalchemy`                                                   | `teradatasql://{user}:{password}@{host}`                                                                                                               |
| [TimescaleDB](/docs/configuration/databases#timescaledb)                | `pip install psycopg2`                                                             | `postgresql://<UserName>:<DBPassword>@<Database Host>:<Port>/<Database Name>`                                                                          |
| [Trino](/docs/configuration/databases#trino)                            | `pip install trino`                                                                | `trino://{username}:{password}@{hostname}:{port}/{catalog}`                                                                                            |
| [Vertica](/docs/configuration/databases#vertica)                        | `pip install sqlalchemy-vertica-python`                                            | `vertica+vertica_python://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                     |
| [YDB](/docs/configuration/databases#ydb)                                | `pip install ydb-sqlalchemy`                                                       | `ydb://{host}:{port}/{database_name}`                                                                                                                  |
| [YugabyteDB](/docs/configuration/databases#yugabytedb)                  | `pip install psycopg2`                                                             | `postgresql://<UserName>:<DBPassword>@<Database Host>/<Database Name>`                                                                                 |

---

他にも多くのデータベースがサポートされていることに注意してください。主な基準は、機能的な SQLAlchemy 方言と Python ドライバーの存在です。キーワード「sqlalchemy + (データベース名)」を検索すると、適切な場所にたどり着くことができます。

データベースまたはデータ エンジンがリストにないけれども、SQL インターフェイスが存在する場合は、[Superset GitHub リポジトリ](https://github.com/apache/superset/issues) に問題を報告してください。そうすれば、ドキュメント化とサポートに取り組むことができます。

Superset 統合用のデータベース コネクタを構築する場合は、[次のチュートリアル](https://preset.io/blog/building-database-connector/) をお読みください。

### Docker イメージにドライバーをインストールする

Superset では、接続する追加のデータベースの種類ごとに Python データベース ドライバーをインストールする必要があります。

この例では、MySQL コネクタライブラリのインストール方法について説明します。
コネクタライブラリのインストールプロセスは、すべての追加ライブラリで同じです。

#### 1. 必要なドライバーを決定する

[データベースドライバ一覧](/docs/configuration/databases)を参照し、データベースへの接続に必要なPyPIパッケージを見つけてください。この例ではMySQLデータベースに接続するため、`mysqlclient`コネクタライブラリが必要になります。

#### 2. コンテナにドライバーをインストールする

`mysqlclient` ライブラリを Superset Docker コンテナにインストールする必要があります (ホスト マシンにインストールされているかどうかは関係ありません)。`docker exec -it <container_name> bash` を使用して実行中のコンテナに入り、そこで `pip install mysqlclient` を実行することもできますが、永続的に保持されるわけではありません。

この問題に対処するために、Superset `docker compose` デプロイメントでは `requirements-local.txt` ファイルの規則を使用します。
このファイルにリストされているすべてのパッケージは、実行時に PyPI からコンテナにインストールされます。
このファイルは、ローカル開発の目的で Git によって無視されます。

`docker-compose.yml` または `docker-compose-non-dev.yml` ファイルがあるディレクトリにある `docker` というサブディレクトリに `requirements-local.txt` ファイルを作成します。

```bash
# Run from the repo root:
touch ./docker/requirements-local.txt
```

上記の手順で特定したドライバーを追加します。
テキストエディタを使用するか、コマンドラインから次のように実行できます:

```bash
echo "mysqlclient" >> ./docker/requirements-local.txt
```

**標準の（カスタマイズされていない）Superset イメージを実行している場合**、これで完了です。
`docker compose -f docker-compose-non-dev.yml up` で Superset を起動すると、ドライバーがインストールされているはずです。

`docker exec -it <container_name> bash` で実行中のコンテナに入り、`pip freeze` を実行することで、ドライバーの存在を確認できます。表示されるリストに PyPI パッケージが含まれているはずです。

**カスタマイズされた Docker イメージを実行している場合**、新しいドライバーを組み込んだローカルイメージを再構築します:

```bash
docker compose build --force-rm
```

Docker イメージの再構築が完了したら、`docker compose up` を実行して Superset を再起動します。

#### 3. MySQLに接続する

コンテナに MySQL ドライバーがインストールされたので、Superset の Web UI 経由でデータベースに接続できるようになります。

管理者ユーザーとして、「Settings」 -> 「Data: Database Connections」に移動し、「+DATABASE」ボタンをクリックします。
そこから、[データベース接続 UI の使用](/docs/configuration/databases#connecting-through-the-ui) の手順に従います。

Superset ドキュメントで、ご利用のデータベースの種類に応じたページを参照し、接続文字列とその他の入力パラメータを確認してください。
例えば、[MySQL ページ](/docs/configuration/databases#mysql) では、ローカル MySQL データベースへの接続文字列は、セットアップが Linux で実行されているか Mac で実行されているかによって異なります。

「Test Connection」 ボタンをクリックすると、「Connection looks good!」というポップアップメッセージが表示されます。

#### 4. トラブルシューティング

テストが失敗した場合は、Docker ログでエラー メッセージを確認してください。Superset は SQLAlchemy を使用してデータベースに接続します。データベースの接続文字列のトラブルシューティングを行うには、Superset アプリケーション コンテナーまたはホスト環境で Python を起動し、目的のデータベースに直接接続してデータを取得してみてください。これにより、問題を切り分けるために Superset が排除されます。

Superset が接続するデータベースの種類ごとにこのプロセスを繰り返します。

### データベース固有の指示

#### Ascend.io

Ascend.io に推奨されるコネクタ ライブラリは [impyla](https://github.com/cloudera/impyla) です。

想定される接続文字列の形式は次のとおりです:

```
ascend://{username}:{password}@{hostname}:{port}/{database}?auth_mechanism=PLAIN;use_ssl=true
```

#### Apache Doris

SQLAlchemy 経由で Apache Doris に接続するには、[sqlalchemy-doris](https://pypi.org/project/pydoris/) ライブラリの使用が推奨されます。

接続文字列を作成するには、以下の設定値が必要です:

- **User**: User Name
- **Password**: Password
- **Host**: Doris FE Host
- **Port**: Doris FE port
- **Catalog**: Catalog Name
- **Database**: Database Name

接続文字列は次のようになります:

```
doris://<User>:<Password>@<Host>:<Port>/<Catalog>.<Database>
```

#### AWS Athena

##### PyAthenaJDBC

[PyAthenaJDBC](https://pypi.org/project/PyAthenaJDBC/) は、[Amazon Athena JDBC ドライバー](https://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html) 用の Python DB 2.0 準拠のラッパーです。

Amazon Athena の接続文字列は次のとおりです:

```
awsathena+jdbc://{aws_access_key_id}:{aws_secret_access_key}@athena.{region_name}.amazonaws.com/{schema_name}?s3_staging_dir={s3_staging_dir}&...
```

次のように接続文字列を形成するときは、エスケープとエンコードを行う必要があることに注意してください:

```
s3://... -> s3%3A//...
```

##### PyAthena

次の接続文字列を使用して、[PyAthena ライブラリ](https://pypi.org/project/PyAthena/) (Java は不要) を使用することもできます:

```
awsathena+rest://{aws_access_key_id}:{aws_secret_access_key}@athena.{region_name}.amazonaws.com/{schema_name}?s3_staging_dir={s3_staging_dir}&...
```

PyAthena ライブラリでは、Superset の Athena データベース接続 UI の [ADVANCED] --> [Other] --> [ENGINE PARAMETERS] で次のパラメータを追加することで定義できる特定の IAM ロールを引き受けることもできます。

```json
{
    "connect_args": {
        "role_arn": "<role arn>"
    }
}
```

#### AWS DynamoDB

##### PyDynamoDB

[PyDynamoDB](https://pypi.org/project/PyDynamoDB/) は、Amazon DynamoDB 用の Python DB API 2.0 (PEP 249) クライアントです。

Amazon DynamoDB への接続文字列は次のとおりです:

```
dynamodb://{aws_access_key_id}:{aws_secret_access_key}@dynamodb.{region_name}.amazonaws.com:443?connector=superset
```

詳細なドキュメントについては、[PyDynamoDB WIKI](https://github.com/passren/PyDynamoDB/wiki/5.-Superset) をご覧ください。

#### AWS Redshift

SQLAlchemy 経由で Redshift に接続するには、[sqlalchemy-redshift](https://pypi.org/project/sqlalchemy-redshift/) ライブラリの使用が推奨されます。

この方言が正しく動作するには、[redshift_connector](https://pypi.org/project/redshift-connector/) または [psycopg2](https://pypi.org/project/psycopg2/) のいずれかが必要です。

接続文字列を作成するには、以下の値を設定する必要があります:

- **User Name**: userName
- **Password**: DBPassword
- **Database Host**: AWS Endpoint
- **Database Name**: Database Name
- **Port**: default 5439

##### psycopg2

SQLALCHEMY URI は次のようになります:

```
redshift+psycopg2://<userName>:<DBPassword>@<AWS End Point>:5439/<Database Name>
```

##### redshift_connector

SQLALCHEMY URI は次のようになります:

```
redshift+redshift_connector://<userName>:<DBPassword>@<AWS End Point>:5439/<Database Name>
```

###### Redshift クラスターで IAM ベースの認証情報を使用する

[Amazon Redshift クラスター](https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html) は、一時的な IAM ベースのデータベースユーザー認証情報の生成もサポートしています。

superset アプリの [IAM ロールには、`redshift:GetClusterCredentials` オペレーションを呼び出すための権限が必要です](https://docs.aws.amazon.com/redshift/latest/mgmt/generating-iam-credentials-role-permissions.html)。

superset の Redshift データベース接続 UI の [ADVANCED] --> [Others] --> [ENGINE PARAMETERS] で、以下の引数を定義する必要があります。

```
{"connect_args":{"iam":true,"database":"<database>","cluster_identifier":"<cluster_identifier>","db_user":"<db_user>"}}
```

SQLALCHEMY URIは`redshift+redshift_connector://`に設定する必要があります。

###### Redshift サーバーレスで IAM ベースの認証情報を使用する

[Redshift Serverless](https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-whatis.html) は、IAM ロールを使用した接続をサポートしています。

Superset アプリの IAM ロールには、Redshift Serverless ワークグループに対する `redshift-serverless:GetCredentials` および `redshift-serverless:GetWorkgroup` 権限が必要です。

Superset の Redshift データベース接続 UI の「ADVANCED」->「Others」->「ENGINE PARAMETERS」で、以下の引数を定義する必要があります。

```
{"connect_args":{"iam":true,"is_serverless":true,"serverless_acct_id":"<aws account number>","serverless_work_group":"<redshift work group>","database":"<database>","user":"IAMR:<superset iam role name>"}}
```

#### ClickHouse

ClickHouse を Superset で使用するには、`clickhouse-connect` Python ライブラリをインストールする必要があります:

Docker Compose を使用して Superset を実行する場合は、`./docker/requirements-local.txt` ファイルに次の内容を追加します:

```
clickhouse-connect>=0.6.8
```

ClickHouse に推奨されるコネクタライブラリは [clickhouse-connect](https://github.com/ClickHouse/clickhouse-connect) です。

想定される接続文字列の形式は次のとおりです:

```
clickhousedb://<user>:<password>@<host>:<port>/<database>[?options…]clickhouse://{username}:{password}@{hostname}:{port}/{database}
```

実際の接続文字列の具体的な例を次に示します:

```
clickhousedb://demo:demo@github.demo.trial.altinity.cloud/default?secure=true
```

Clickhouse をコンピュータ上でローカルに使用している場合は、パスワードなしでデフォルトのユーザーを使用する (接続を暗号化しない) http プロトコル URL を使用すれば済みます:

```
clickhousedb://localhost/default
```

#### CockroachDB

CockroachDB に推奨されるコネクタライブラリは [sqlalchemy-cockroachdb](https://github.com/cockroachdb/sqlalchemy-cockroachdb) です。

想定される接続文字列の形式は次のとおりです:

```
cockroachdb://root@{hostname}:{port}/{database}?sslmode=disable
```

#### Couchbase

Couchbase の Superset 接続は、Couchbase Analytics と Couchbase Columnar の 2 つのサービスをサポートするように設計されています。
Couchbase に推奨されるコネクタライブラリは [couchbase-sqlalchemy](https://github.com/couchbase/couchbase-sqlalchemy) です。

```
pip install couchbase-sqlalchemy
```

予想される接続文字列の形式は次のとおりです:

```
couchbase://{username}:{password}@{hostname}:{port}?truststorepath={certificate path}?ssl={true/false}
```

#### CrateDB

CrateDB のコネクタライブラリは [sqlalchemy-cratedb] です。
`requirements.txt` ファイルに次の項目を追加することをお勧めします:

```
sqlalchemy-cratedb>=0.40.1,<1
```

評価目的でローカルホスト上の[CrateDB Self-Managed]のSQLAlchemy接続文字列は次のようになります:

```
crate://crate@127.0.0.1:4200
```

[CrateDB Cloud] に接続するための SQLAlchemy 接続文字列は次のようになります:

```
crate://<username>:<password>@<clustername>.cratedb.net:4200/?ssl=true
```

Docker Compose を使用してローカルで Superset を設定する場合は、[こちら](/docs/configuration/databases#installing-database-drivers) の手順に従って、CrateDB コネクタ パッケージをインストールしてください。

```
echo "sqlalchemy-cratedb" >> ./docker/requirements-local.txt
```

[CrateDB Cloud]: https://cratedb.com/product/cloud
[CrateDB Self-Managed]: https://cratedb.com/product/self-managed
[sqlalchemy-cratedb]: https://pypi.org/project/sqlalchemy-cratedb/

#### Databend

Databend の推奨コネクタライブラリは [databend-sqlalchemy](https://pypi.org/project/databend-sqlalchemy/) です。
Superset は `databend-sqlalchemy>=0.2.3` でテストされています。

推奨される接続文字列は次のとおりです。

```
databend://{username}:{password}@{host}:{port}/{database_name}
```

以下は、Superset が Databend データベースに接続する接続文字列の例です:

```
databend://user:password@localhost:8000/default?secure=false
```

#### Databricks

Databricks は現在、ネイティブ DB API 2.0 ドライバ「databricks-sql-connector」を提供しています。これは「sqlalchemy-databricks」方言で使用できます。どちらも以下のコマンドでインストールできます:

```bash
pip install "apache-superset[databricks]"
```

Hive コネクタを使用するには、クラスターから次の情報を取得する必要があります:

- Server hostname
- Port
- HTTP path

これらは、「構成」→「詳細オプション」→「JDBC/ODBC」にあります。

また、「設定」→「ユーザー設定」→「アクセストークン」からアクセストークンを取得する必要があります。

これらの情報をすべて入手したら、「Databricks Native Connector」タイプのデータベースを追加し、以下のSQLAlchemy URIを使用します:

```
databricks+connector://token:{access_token}@{server_hostname}:{port}/{database_name}
```

また、HTTP パスを使用して、「その他」->「エンジン パラメータ」に次の構成を追加する必要があります:

```json
{
    "connect_args": {"http_path": "sql/protocolv1/o/****"}
}
```

##### Older driver

元々、SupersetはDatabricksへの接続に`databricks-dbapi`を使用していました。公式のDatabricksコネクタで問題が発生している場合は、こちらを試してみると良いかもしれません:

```bash
pip install "databricks-dbapi[sqlalchemy]"
```

`databricks-dbapi` を使用する場合、Databricks に接続する方法は 2 つあります。Hive コネクタを使用する方法と ODBC コネクタを使用する方法です。どちらの方法も動作は似ていますが、[SQL エンドポイント](https://docs.databricks.com/sql/admin/sql-endpoints.html) への接続には ODBC のみを使用できます。

#### Hive

Hive クラスターに接続するには、Superset に「Databricks Interactive Cluster」タイプのデータベースを追加し、次の SQLAlchemy URI を使用します:

```
databricks+pyhive://token:{access_token}@{server_hostname}:{port}/{database_name}
```

また、HTTP パスを使用して、「Other」->「Engine Parameters」に次の構成を追加する必要があります。

```json
{"connect_args": {"http_path": "sql/protocolv1/o/****"}}
```

#### ODBC

ODBC を使用する場合は、まず [お使いのプラットフォーム用の ODBC ドライバー](https://databricks.com/spark/odbc-drivers-download) をインストールする必要があります。

通常の接続の場合は、ユースケースに応じてデータベースとして「Databricks Interactive Cluster」または「Databricks SQL Endpoint」を選択した後、これを SQLAlchemy URI として使用します:

```
databricks+pyodbc://token:{access_token}@{server_hostname}:{port}/{database_name}
```

接続引数については次のようになります:

```json
{"connect_args": {"http_path": "sql/protocolv1/o/****", "driver_path": "/path/to/odbc/driver"}}
```

ドライバーのパスは次のようになります:

- `/Library/simba/spark/lib/libsparkodbc_sbu.dylib` (Mac OS)
- `/opt/simba/spark/lib/64/libsparkodbc_sb64.so` (Linux)

SQL エンドポイントに接続するには、エンドポイントからの HTTP パスを使用する必要があります:

```json
{"connect_args": {"http_path": "/sql/1.0/endpoints/****", "driver_path": "/path/to/odbc/driver"}}
```

#### Denodo

Denodo に推奨されるコネクタライブラリは [denodo-sqlalchemy](https://pypi.org/project/denodo-sqlalchemy/) です。

想定される接続文字列の形式は次のとおりです（デフォルトポートは 9996 です）:

```
denodo://{username}:{password}@{hostname}:{port}/{database}
```

#### Dremio

Dremio に推奨されるコネクタライブラリは [sqlalchemy_dremio](https://pypi.org/project/sqlalchemy-dremio/) です。

ODBC の接続文字列（デフォルトポートは 31010）は、以下の形式になります:

```
dremio+pyodbc://{username}:{password}@{host}:{port}/{database_name}/dremio?SSL=1
```

Arrow Flight (Dremio 4.9.1+。デフォルト ポートは 32010) の予想される接続文字列は、次の形式になります:

```
dremio+flight://{username}:{password}@{host}:{port}/dremio
```

この [Dremio のブログ投稿](https://www.dremio.com/tutorials/dremio-apache-superset/) には、Superset を Dremio に接続するための追加の役立つ手順が記載されています。

#### Apache Drill

##### SQLAlchemy

Apache Drillへの接続には、SQLAlchemy経由の接続が推奨されます。[sqlalchemy-drill](https://github.com/JohnOmernik/sqlalchemy-drill) パッケージを使用できます。

インストールが完了すると、RESTインターフェース経由またはJDBC経由の2つの方法でDrillに接続できるようになります。
JDBC経由で接続する場合は、Drill JDBCドライバーがインストールされている必要があります。

Drillの基本的な接続文字列は次のとおりです。

```
drill+sadrill://<username>:<password>@<host>:<port>/<storage_plugin>?use_ssl=True
```

組み込みモードで実行されているローカル マシンで実行されている Drill に接続するには、次の接続文字列を使用できます:

```
drill+sadrill://localhost:8047/dfs?use_ssl=False
```

##### JDBC

JDBC 経由で Drill に接続する方法はより複雑なため、[こちらのチュートリアル](https://drill.apache.org/docs/using-the-jdbc-driver/) に従うことをお勧めします。

接続文字列は次のようになります:

```
drill+jdbc://<username>:<password>@<host>:<port>
```

##### ODBC

ODBC を介して Drill を操作する方法を学ぶには、[Apache Drill のドキュメント](https://drill.apache.org/docs/installing-the-driver-on-linux/) を読み、[GitHub README](https://github.com/JohnOmernik/sqlalchemy-drill#usage-with-odbc) を読むことをお勧めします。

import useBaseUrl from "@docusaurus/useBaseUrl";

#### Apache Druid

Druid へのネイティブ コネクタは Superset (`DRUID_IS_ACTIVE` フラグの背後) に同梱されていますが、これは [pydruid ライブラリ](https://pythonhosted.org/pydruid/) で利用できる SQLAlchemy / DBAPI コネクタに取って代わられ、徐々に廃止されつつあります。

接続文字列は次のようになります:

```
druid://<User>:<password>@<Host>:<Port-default-9088>/druid/v2/sql
```

この接続文字列の主要な構成要素は次のとおりです。

- `User`: データベースへの接続に必要な認証情報のユーザー名部分
- `Password`: データベースへの接続に必要な認証情報のパスワード部分
- `Host`: データベースを実行しているホストマシンのIPアドレス（またはURL）
- `Port`: データベースを実行しているホストマシンで公開されている特定のポート

##### Druid 接続のカスタマイズ

Druid への接続を追加する際、**Add Database** フォームでいくつかの方法で接続をカスタマイズできます。

**カスタム証明書**

Druid への新しいデータベース接続を構成する際に、**Root Certificate** フィールドに証明書を追加できます:

<img src={useBaseUrl("/img/root-cert-example.png")} />{" "}

カスタム証明書を使用する場合、pydruid は自動的に https スキームを使用します。

**SSL 検証を無効にする**

SSL 検証を無効にするには、**Extras** フィールドに以下のコードを追加します:

```
engine_params:
{"connect_args":
 {"scheme": "https", "ssl_verify_cert": false}}
```

##### 集約

Superset では、一般的な集計や Druid メトリクスを定義・使用できます。最初の、そしてよりシンプルなユースケースは、データソースの編集ビューに表示されるチェックボックスマトリックスを使用することです (**Sources -> Druid Datasources -> [your datasource] -> Edit -> [tab] [List Druid Column]**)。

[GroupBy] および [Filterable] チェックボックスをオンにすると、Explore ビューで関連するドロップダウンに列が表示されます。[Count Distinct]、[Min]、[Max]、または [Sum] をオンにすると、新しいメトリクスが作成され、データソースの保存時に **List Druid Metric** タブに表示されます。

これらのメトリクスを編集すると、その JSON 要素が Druid 集計定義に対応していることがわかります。**List Druid Metric** タブから、Druid ドキュメントに従って独自の集計を手動で作成することもできます。

##### 集約後

Druidは集約後をサポートし、これはSuperSetで機能します。あなたがしなければならないのは、あなたが手動で集約を作成するのと同じようにメトリックを作成することですが、「Postagg」を「メトリックタイプ」として指定します。次に、JSONフィールドで有効なJSON後の凝集後定義（ドルイドドキュメントで指定）を提供する必要があります。

#### Elasticsearch

ElasticSearchの推奨コネクタライブラリは[Elasticsearch-dbapi]（https://github.com/preset-io/elasticsearch-dbapi）です。

ElasticSearchの接続文字列は次のようになります：

```
elasticsearch+http://{user}:{password}@{host}:9200/
```

**HTTPSを使用**

```
elasticsearch+https://{user}:{password}@{host}:9200/
```

ElasticSearchは10000行のデフォルト制限として、クラスターのこの制限を増やしたり、SuperSetの行の制限を設定したりすることができます。

```
ROW_LIMIT = 10000
```

たとえば、SQL Labで複数のインデックスをクエリすることができます

```
SELECT timestamp, agent FROM "logstash"
```

ただし、複数のインデックスに視覚化を使用するには、クラスターにエイリアスインデックスを作成する必要があります

```
POST /_aliases
{
    "actions" : [
        { "add" : { "index" : "logstash-**", "alias" : "logstash_all" } }
    ]
}
```

次に、エイリアス名logstash_allにテーブルを登録します

**タイムゾーン**

デフォルトでは、SuperSetはElasticSearchクエリにUTCタイムゾーンを使用します。タイムゾーンを指定する必要がある場合は、データベースを編集し、指定されたタイムゾーンの設定を Other > ENGINE PARAMETERS に入力してください:

```json
{
    "connect_args": {
        "time_zone": "Asia/Shanghai"
    }
}
```

タイムゾーンの問題について注意すべきもう1つの問題は、Elasticsearch7.8の前に、文字列を「datetime」オブジェクトに変換する場合、「cast」関数を使用する必要があるが、この関数は「time_zone」設定をサポートしないことです。したがって、ElasticSearch7.8の後にバージョンにアップグレードすることをお勧めします。
ElasticSearch7.8の後、「datetime_parse`関数を使用してこの問題を解決できます。
DateTime_Parse関数は、「time_zone」設定をサポートすることであり、ここでは、他の>バージョン設定でElasticSearchバージョン番号を入力する必要があります。
superset は、変換に「datetime_parse」関数を使用します。

**SSL検証を無効にします**

SSL検証を無効にするには、**SQLALCHEMY URI** フィールドに以下を追加します。

```
elasticsearch+https://{user}:{password}@{host}:9200/?verify_certs=False
```

#### Exasol

Exasolの推奨コネクタライブラリは[Sqlalchemy-exasol]（https://github.com/exasol/sqlalchemy-exasol）です。

Exasolの接続文字列は次のようになります：

```
exa+pyodbc://{username}:{password}@{hostname}:{port}/my_schema?CONNECTIONLCALL=en_US.UTF-8&driver=EXAODBC
```

#### Firebird

Firebirdの推奨コネクタライブラリは[sqlalchemy-firebird]（https://pypi.org/project/sqlalchemy-firebird/）です。
スーパーセットは、 `sqlalchemy-firebird> = 0.7.0、<0.8`でテストされています。

推奨される接続文字列は次のとおりです:

```
firebird+fdb://{username}:{password}@{host}:{port}//{path_to_db_file}
```

これは、ローカルファイアバードデータベースに接続する Superset の接続文字列の例です。

```
firebird+fdb://SYSDBA:masterkey@192.168.86.38:3050//Library/Frameworks/Firebird.framework/Versions/A/Resources/examples/empbuild/employee.fdb
```

#### Firebolt

Fireboltの推奨コネクタライブラリは[Firebolt-sqlalchemy]（https://pypi.org/project/firebolt-sqlalchemy/）です。

推奨される接続文字列は次のとおりです:

```
firebolt://{username}:{password}@{database}?account_name={name}
または
firebolt://{username}:{password}@{database}/{engine_name}?account_name={name}
```

サービスアカウントを使用して接続することも可能です:

```
firebolt://{client_id}:{client_secret}@{database}?account_name={name}
または
firebolt://{client_id}:{client_secret}@{database}/{engine_name}?account_name={name}
```

#### Google BigQuery

BigQueryの推奨コネクタライブラリは [sqlalchemy-bigquery](https://github.com/googleapis/python-bigquery-sqlalchemy) です。

##### BigQueryドライバーをインストールします

Docker Composeを介してSuperSetをローカルにセットアップするときに新しいデータベースドライバーをインストールする方法については、[こちら](/docs/configuration/databases#installing-drivers-in-docker-images) に従ってください。

```bash
echo "sqlalchemy-bigquery" >> ./docker/requirements-local.txt
```

##### BigQueryへの接続

SuperSetに新しいBigQuery接続を追加するときは、GCP Service Account Sredentialsファイル（JSONとして）を追加する必要があります。

1. Google Cloud Platformコントロールパネルを介してサービスアカウントを作成し、適切なBigQueryデータセットへのアクセスを提供し、サービスアカウントのJSON構成ファイルをダウンロードします。
2. Superset では、そのJSONをアップロードするか、次の形式でJSON Blobを追加することができます（これは資格情報JSONファイルのコンテンツになります）:

```json
{
        "type": "service_account",
        "project_id": "...",
        "private_key_id": "...",
        "private_key": "...",
        "client_email": "...",
        "client_id": "...",
        "auth_uri": "...",
        "token_uri": "...",
        "auth_provider_x509_cert_url": "...",
        "client_x509_cert_url": "..."
    }
```

![CleanShot 2021-10-22 at 04 18 11](https://user-images.githubusercontent.com/52086618/138352958-a18ef9cb-8880-4ef1-88c1-452a9f1b8105.gif)

3. さらに、代わりにSqlalchemy URIを介して接続できます

   BigQueryの接続文字列は次のように見えます：

   ```
   bigquery://{project_id}
   ```

   **Advanced**タブに移動し、次の形式でデータベース構成フォームの **Secure Extra** フィールドにJSONブロブを追加します。

   ```json
   {
   "credentials_info": <contents of credentials JSON file>
   }
   ```

   結果のファイルには、この構造が必要です:

   ```json
   {
    "credentials_info": {
        "type": "service_account",
        "project_id": "...",
        "private_key_id": "...",
        "private_key": "...",
        "client_email": "...",
        "client_id": "...",
        "auth_uri": "...",
        "token_uri": "...",
        "auth_provider_x509_cert_url": "...",
        "client_x509_cert_url": "..."
        }
    }
   ```

その後、BigQueryデータセットに接続できるはずです。

![CleanShot 2021-10-22 at 04 47 08](https://user-images.githubusercontent.com/52086618/138354340-df57f477-d3e5-42d4-b032-d901c69d2213.gif)

SuperSetのBigQueryにCSVまたはExcelファイルをアップロードできるようにするには、[pandas_gbq]（https://github.com/pydata/pandas-gbq）ライブラリを追加する必要があります。

現在、Google Bigquery Python SDKは、「Gevent」によるPython Coreライブラリでの動的なモンキーパッチがあるため、「Gevent」と互換性がありません。
したがって、「Gunicorn」サーバーでSuperSetを展開する場合、「Gevent」を除くワーカータイプを使用する必要があります。

#### Google Sheets

Googleシートには非常に限られた[SQL API](https://developers.google.com/chart/interactive/docs/querylanguage) があります。 Googleシート用の推奨コネクタライブラリは [Shillelagh](https://github.com/betodealmeida/shillelagh) です。

SuperSetをGoogleシートに接続することには、いくつかの手順があります。この[チュートリアル](https://preset.io/blog/2020-06-01-connect-superset-google-sheets/) には、この接続の設定に関する最新の指示があります。

#### Hana

推奨されるコネクタライブラリは[sqlalchemy-hana](https://github.com/sap/sqlalchemy-hana) です。

接続文字列は次のようにフォーマットされます:

```
hana://{username}:{password}@{host}:{port}
```

#### Apache Hive

[pyhive](https://pypi.org/project/pyhive/) ライブラリは、sqlalchemyを通じてハイブに接続する推奨方法です。

予想される接続文字列は、次のようにフォーマットされます:

```
hive://hive@{hostname}:{port}/{database}
```

#### Hologres

Hologres は、Alibaba Cloud が開発したリアルタイムのインタラクティブな分析サービスです。 PostgreSQL 11 と完全に互換性があり、BigData エコシステムとシームレスに統合します。

Hologres サンプル接続パラメーター：

- **User Name**: The AccessKey ID of your Alibaba Cloud account.
- **Password**: The AccessKey secret of your Alibaba Cloud account.
- **Database Host**: The public endpoint of the Hologres instance.
- **Database Name**: The name of the Hologres database.
- **Port**: The port number of the Hologres instance.

接続文字列は次のようになります:

```
postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}
```

#### IBM DB2

[IBM_DB_SA](https://github.com/ibmdb/python-ibmdbsa/master/ibm_db_sa) ライブラリは、IBM データサーバーに Python/SQLAlchemy インターフェイスを提供します。

これが推奨される接続文字列です：

```
db2+ibm_db://{username}:{passport}@{hostname}:{port}/{database}
```

Sqlalchemyに実装されている2つのDB2方言バージョンがあります。 `limit [n]` syntaxなしでdb2バージョンに接続している場合、SQLラボを使用できるように推奨される接続文字列は次のとおりです:

```
ibm_db_sa://{username}:{passport}@{hostname}:{port}/{database}
```

#### Apache Impala

Apache Impalaへの推奨コネクタライブラリは [Impyla](https://github.com/cloudera/impyla) です。

予想される接続文字列は、次のようにフォーマットされます:

```
impala://{hostname}:{port}/{database}
```

#### Kusto

Kustoの推奨コネクタライブラリは [sqlalchemy-kusto](https://pypi.org/project/sqlalchemy-kusto/2.0.0/) > = 2.0.0 です。

Kusto（sql dialect）の接続文字列は次のようになります:

```
kustosql+https://{cluster_url}/{database}?azure_ad_client_id={azure_ad_client_id}&azure_ad_client_secret={azure_ad_client_secret}&azure_ad_tenant_id={azure_ad_tenant_id}&msi=False
```

Kusto（kql dialect）の接続文字列は次のようになります。

```
kustokql+https://{cluster_url}/{database}?azure_ad_client_id={azure_ad_client_id}&azure_ad_client_secret={azure_ad_client_secret}&azure_ad_tenant_id={azure_ad_tenant_id}&msi=False
```

ユーザーがすべての必要なデータベース/テーブル/ビューにアクセスして使用する特権を持っていることを確認してください。

#### Apache Kylin

Apache Kylinの推奨コネクタライブラリは[Kylinpy]（https://github.com/kyligence/kylinpy）です。

予想される接続文字列は、次のようにフォーマットされます:

```
kylin://<username>:<password>@<hostname>:<port>/<project>?<param1>=<value1>&<param2>=<value2>
```

#### MySQL

mysqlの推奨コネクタライブラリは [mysqlclient](https://pypi.org/project/mysqlclient/) です。

これが接続文字列です:

```
mysql://{username}:{password}@{host}/{database}
```

Host:

- For Localhost: `localhost` or `127.0.0.1`
- Docker running on Linux: `172.18.0.1`
- For On Prem: IP address or Host name
- For Docker running in OSX: `docker.for.mac.host.internal`
  Port: `3306` by default

「mysqlclient」の問題の1つは、プラグインがクライアントに含まれていないため、認証用に「caching_sha2_password」を使用して新しいmysqlデータベースに接続できないことです。この場合、[mysql-connector-python](https://pypi.org/project/mysql-connector-python/) を使用する必要があります。

```
mysql+mysqlconnector://{username}:{password}@{host}/{database}
```

#### IBM Netezza Performance Server

[nzalchemy](https://pypi.org/project/nzalchemy/) ライブラリは、IBM Netezza Performance Server（別名Netezza）にPython/SQLAlchemyインターフェイスを提供します。

これが推奨される接続文字列です:

```
netezza+nzpy://{username}:{password}@{hostname}:{port}/{database}
```

#### OceanBase

[sqlalchemy-oceanbase](https://pypi.org/project/oceanbase_py/) ライブラリは、SQLAlchemy を通じて OceanBase に接続する推奨方法です。

OceanBase の接続文字列は次のようになります：

```
oceanbase://<User>:<Password>@<Host>:<Port>/<Database>
```

#### Ocient DB

Ocientの推奨コネクタライブラリは[sqlalchemy-ocient]（https://pypi.org/project/sqlalchemy-ocient）です。

##### Ocient ドライバーのインストール

```bash
pip install sqlalchemy-ocient
```

##### COcient への接続

Ocient DSN の形式は次のとおりです:

```shell
ocient://user:password@[host][:port][/database][?param1=value1&...]
```

#### Oracle

推奨されるコネクタライブラリは [cx_oracle](https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html) です。

接続文字列は次のようにフォーマットされます。

```
oracle://<username>:<password>@<hostname>:<port>
```

#### Parseable

[Parsable](https://www.parsable.io) は、ログデータに SQL のようなクエリインターフェイスを提供する分散ログ分析データベースです。推奨されるコネクタライブラリは [sqlalchemy-parsable](https://github.com/parsablehq/sqlalchemy-parsable) です。

接続文字列は次のようにフォーマットされます:

```
parseable://<username>:<password>@<hostname>:<port>/<stream_name>
```

例えば:

```
parseable://admin:admin@demo.parseable.com:443/ingress-nginx
```

注：URI の stream_name は、クエリする断続的なログストリームを表します。 HTTP（ポート80）とHTTPS（ポート443）接続の両方を使用できます。

>>>>>>>
#### Apache Pinot

Apache Pinotの推奨コネクタライブラリは[pinotdb](https://pypi.org/project/pinotdb/) です。

予想される接続文字列は、次のようにフォーマットされます:

```
pinot+http://<pinot-broker-host>:<pinot-broker-port>/query?controller=http://<pinot-controller-host>:<pinot-controller-port>/``
```

ユーザー名とパスワードを使用した予想される接続文字列は、次のようにフォーマットされます:

```
pinot://<username>:<password>@<pinot-broker-host>:<pinot-broker-port>/query/sql?controller=http://<pinot-controller-host>:<pinot-controller-port>/verify_ssl=true``
```

ビューや結合、ウィンドウ関数などをExploreで使用する場合は、[マルチステージクエリエンジン](https://docs.pinot.apache.org/reference/multi-stage-engine)を有効にします。
Advanced -> Other -> ENGINE PARAMETERS でデータベース接続を作成しながら、以下の引数を追加します

```json
{"connect_args":{"use_multistage_engine":"true"}}
```

#### Postgres

Docker Composeを使用している場合、Postgres コネクタライブラリ [psycopg2](https://www.psycopg.org/docs/) が Superset の箱から出てくることに注意してください。

Postgres sample connection parameters:

- **User Name**: UserName
- **Password**: DBPassword
- **Database Host**:
  - For Localhost: localhost or 127.0.0.1
  - For On Prem: IP address or Host name
  - For AWS Endpoint
- **Database Name**: Database Name
- **Port**: default 5432

接続文字列は次のようになります:

```
postgresql://{username}:{password}@{host}:{port}/{database}
```

最後に `？sslmode = require`を追加することで、SSLを要求できます:

```
postgresql://{username}:{password}@{host}:{port}/{database}?sslmode=require
```

[このドキュメントの表 31-1 ](https://www.postgresql.org/docs/9.1/libpq-ssl.html) でサポートする他の SSL モードについて読むことができます。

PostgreSQL接続オプションの詳細については、[SQLAlchemy のドキュメント](https://docs.sqlalchemy.org/en/13/dialects/postgresql.html#module-sqlalchemy.dialects.postgresql.postgg2) および [PostgreSQL のドキュメント](https://www.postgresql.org/docs/9.1/libpq-connect.html#LIBPQ-PQCONNECTDBPARAMS) を参照。

#### Presto

The [pyhive](https://pypi.org/project/PyHive/) library is the recommended way to connect to Presto through SQLAlchemy.

The expected connection string is formatted as follows:

```
presto://{hostname}:{port}/{database}
```

You can pass in a username and password as well:

```
presto://{username}:{password}@{hostname}:{port}/{database}
```

Here is an example connection string with values:

```
presto://datascientist:securepassword@presto.example.com:8080/hive
```

By default Superset assumes the most recent version of Presto is being used when querying the
datasource. If you’re using an older version of Presto, you can configure it in the extra parameter:

```json
{
    "version": "0.123"
}
```

SSL Secure extra add json config to extra connection information.

```json
   {
     "connect_args":
    {"protocol": "https",
     "requests_kwargs":{"verify":false}
  }
}
```

#### RisingWave

The recommended connector library for RisingWave is
[sqlalchemy-risingwave](https://github.com/risingwavelabs/sqlalchemy-risingwave).

The expected connection string is formatted as follows:

```
risingwave://root@{hostname}:{port}/{database}?sslmode=disable
```

#### Rockset

The connection string for Rockset is:

```
rockset://{api key}:@{api server}
```

Get your API key from the [Rockset console](https://console.rockset.com/apikeys).
Find your API server from the [API reference](https://rockset.com/docs/rest-api/#introduction). Omit the `https://` portion of the URL.

To target to a specific virtual instance, use this URI format:

```
rockset://{api key}:@{api server}/{VI ID}
```

For more complete instructions, we recommend the [Rockset documentation](https://docs.rockset.com/apache-superset/).

#### Snowflake

##### Install Snowflake Driver

Follow the steps [here](/docs/configuration/databases#installing-database-drivers) about how to
install new database drivers when setting up Superset locally via docker compose.

```bash
echo "snowflake-sqlalchemy" >> ./docker/requirements-local.txt
```

The recommended connector library for Snowflake is
[snowflake-sqlalchemy](https://pypi.org/project/snowflake-sqlalchemy/).

The connection string for Snowflake looks like this:

```
snowflake://{user}:{password}@{account}.{region}/{database}?role={role}&warehouse={warehouse}
```

The schema is not necessary in the connection string, as it is defined per table/query. The role and
warehouse can be omitted if defaults are defined for the user, i.e.

```
snowflake://{user}:{password}@{account}.{region}/{database}
```

Make sure the user has privileges to access and use all required
databases/schemas/tables/views/warehouses, as the Snowflake SQLAlchemy engine does not test for
user/role rights during engine creation by default. However, when pressing the “Test Connection”
button in the Create or Edit Database dialog, user/role credentials are validated by passing
“validate_default_parameters”: True to the connect() method during engine creation. If the user/role
is not authorized to access the database, an error is recorded in the Superset logs.

And if you want connect Snowflake with [Key Pair Authentication](https://docs.snowflake.com/en/user-guide/key-pair-auth.html#step-6-configure-the-snowflake-client-to-use-key-pair-authentication).
Please make sure you have the key pair and the public key is registered in Snowflake.
To connect Snowflake with Key Pair Authentication, you need to add the following parameters to "SECURE EXTRA" field.

***Please note that you need to merge multi-line private key content to one line and insert `\n` between each line***

```json
{
     "auth_method": "keypair",
     "auth_params": {
         "privatekey_body": "-----BEGIN ENCRYPTED PRIVATE KEY-----\n...\n...\n-----END ENCRYPTED PRIVATE KEY-----",
         "privatekey_pass":"Your Private Key Password"
     }
 }
```

If your private key is stored on server, you can replace "privatekey_body" with “privatekey_path” in parameter.

```json
{
    "auth_method": "keypair",
    "auth_params": {
        "privatekey_path":"Your Private Key Path",
        "privatekey_pass":"Your Private Key Password"
    }
}
```

#### Apache Solr

The [sqlalchemy-solr](https://pypi.org/project/sqlalchemy-solr/) library provides a
Python / SQLAlchemy interface to Apache Solr.

The connection string for Solr looks like this:

```
solr://{username}:{password}@{host}:{port}/{server_path}/{collection}[/?use_ssl=true|false]
```

#### Apache Spark SQL

The recommended connector library for Apache Spark SQL [pyhive](https://pypi.org/project/PyHive/).

The expected connection string is formatted as follows:

```
hive://hive@{hostname}:{port}/{database}
```

#### SQL Server

The recommended connector library for SQL Server is [pymssql](https://github.com/pymssql/pymssql).

The connection string for SQL Server looks like this:

```
mssql+pymssql://<Username>:<Password>@<Host>:<Port-default:1433>/<Database Name>
```

It is also possible to connect using [pyodbc](https://pypi.org/project/pyodbc) with the parameter [odbc_connect](https://docs.sqlalchemy.org/en/14/dialects/mssql.html#pass-through-exact-pyodbc-string)

The connection string for SQL Server looks like this:

```
mssql+pyodbc:///?odbc_connect=Driver%3D%7BODBC+Driver+17+for+SQL+Server%7D%3BServer%3Dtcp%3A%3Cmy_server%3E%2C1433%3BDatabase%3Dmy_database%3BUid%3Dmy_user_name%3BPwd%3Dmy_password%3BEncrypt%3Dyes%3BConnection+Timeout%3D30
```

#### StarRocks

The [sqlalchemy-starrocks](https://pypi.org/project/starrocks/) library is the recommended
way to connect to StarRocks through SQLAlchemy.

You'll need to the following setting values to form the connection string:

- **User**: User Name
- **Password**: DBPassword
- **Host**: StarRocks FE Host
- **Catalog**: Catalog Name
- **Database**: Database Name
- **Port**: StarRocks FE port

Here's what the connection string looks like:

```
starrocks://<User>:<Password>@<Host>:<Port>/<Catalog>.<Database>
```

:::note
StarRocks maintains their Superset docuementation [here](https://docs.starrocks.io/docs/integrations/BI_integrations/Superset/).
:::

#### TDengine

[TDengine](https://www.tdengine.com) is a High-Performance, Scalable Time-Series Database for Industrial IoT and provides SQL-like query interface.

The recommended connector library for TDengine is [taospy](https://pypi.org/project/taospy/) and [taos-ws-py](https://pypi.org/project/taos-ws-py/)

The expected connection string is formatted as follows:

```
taosws://<user>:<password>@<host>:<port>
```

For example:

```
taosws://root:taosdata@127.0.0.1:6041
```

#### Teradata

The recommended connector library is
[teradatasqlalchemy](https://pypi.org/project/teradatasqlalchemy/).

The connection string for Teradata looks like this:

```
teradatasql://{user}:{password}@{host}
```

#### ODBC Driver

There's also an older connector named
 [sqlalchemy-teradata](https://github.com/Teradata/sqlalchemy-teradata) that
 requires the installation of ODBC drivers. The Teradata ODBC Drivers
 are available
here: https://downloads.teradata.com/download/connectivity/odbc-driver/linux

Here are the required environment variables:

```bash
export ODBCINI=/.../teradata/client/ODBC_64/odbc.ini
export ODBCINST=/.../teradata/client/ODBC_64/odbcinst.ini
```

We recommend using the first library because of the
 lack of requirement around ODBC drivers and
 because it's more regularly updated.

#### TimescaleDB

[TimescaleDB](https://www.timescale.com) is the open-source relational database for time-series and analytics to build powerful data-intensive applications.
TimescaleDB is a PostgreSQL extension, and you can use the standard PostgreSQL connector library, [psycopg2](https://www.psycopg.org/docs/), to connect to the database.

If you're using docker compose, psycopg2 comes out of the box with Superset.

TimescaleDB sample connection parameters:

- **User Name**: User
- **Password**: Password
- **Database Host**:
  - For Localhost: localhost or 127.0.0.1
  - For On Prem: IP address or Host name
  - For [Timescale Cloud](https://console.cloud.timescale.com) service: Host name
  - For [Managed Service for TimescaleDB](https://portal.managed.timescale.com) service: Host name
- **Database Name**: Database Name
- **Port**: default 5432 or Port number of the service

The connection string looks like:

```
postgresql://{username}:{password}@{host}:{port}/{database name}
```

You can require SSL by adding `?sslmode=require` at the end (e.g. in case you use [Timescale Cloud](https://www.timescale.com/cloud)):

```
postgresql://{username}:{password}@{host}:{port}/{database name}?sslmode=require
```

[Learn more about TimescaleDB!](https://docs.timescale.com/)

#### Trino

Supported trino version 352 and higher

##### Connection String

The connection string format is as follows:

```
trino://{username}:{password}@{hostname}:{port}/{catalog}
```

If you are running Trino with docker on local machine, please use the following connection URL

```
trino://trino@host.docker.internal:8080
```

##### Authentications

###### 1. Basic Authentication

You can provide `username`/`password` in the connection string or in the `Secure Extra` field at `Advanced / Security`

- In Connection String

    ```
    trino://{username}:{password}@{hostname}:{port}/{catalog}
    ```

- In `Secure Extra` field

    ```json
    {
        "auth_method": "basic",
        "auth_params": {
            "username": "<username>",
            "password": "<password>"
        }
    }
    ```

NOTE: if both are provided, `Secure Extra` always takes higher priority.

###### 2. Kerberos Authentication

In `Secure Extra` field, config as following example:

```json
{
    "auth_method": "kerberos",
    "auth_params": {
        "service_name": "superset",
        "config": "/path/to/krb5.config",
        ...
    }
}
```

All fields in `auth_params` are passed directly to the [`KerberosAuthentication`](https://github.com/trinodb/trino-python-client/blob/0.306.0/trino/auth.py#L40) class.

NOTE: Kerberos authentication requires installing the [`trino-python-client`](https://github.com/trinodb/trino-python-client) locally with either the `all` or `kerberos` optional features, i.e., installing `trino[all]` or `trino[kerberos]` respectively.

###### 3. Certificate Authentication

In `Secure Extra` field, config as following example:

```json
{
    "auth_method": "certificate",
    "auth_params": {
        "cert": "/path/to/cert.pem",
        "key": "/path/to/key.pem"
    }
}
```

All fields in `auth_params` are passed directly to the [`CertificateAuthentication`](https://github.com/trinodb/trino-python-client/blob/0.315.0/trino/auth.py#L416) class.

###### 4. JWT Authentication

Config `auth_method` and provide token in `Secure Extra` field

```json
{
    "auth_method": "jwt",
    "auth_params": {
        "token": "<your-jwt-token>"
    }
}
```

###### 5. Custom Authentication

To use custom authentication, first you need to add it into
`ALLOWED_EXTRA_AUTHENTICATIONS` allow list in Superset config file:

```python
from your.module import AuthClass
from another.extra import auth_method

ALLOWED_EXTRA_AUTHENTICATIONS: Dict[str, Dict[str, Callable[..., Any]]] = {
    "trino": {
        "custom_auth": AuthClass,
        "another_auth_method": auth_method,
    },
}
```

Then in `Secure Extra` field:

```json
{
    "auth_method": "custom_auth",
    "auth_params": {
        ...
    }
}
```

You can also use custom authentication by providing reference to your `trino.auth.Authentication` class
or factory function (which returns an `Authentication` instance) to `auth_method`.

All fields in `auth_params` are passed directly to your class/function.

**Reference**:

- [Trino-Superset-Podcast](https://trino.io/episodes/12.html)

#### Vertica

The recommended connector library is
[sqlalchemy-vertica-python](https://pypi.org/project/sqlalchemy-vertica-python/). The
[Vertica](http://www.vertica.com/) connection parameters are:

- **User Name:** UserName
- **Password:** DBPassword
- **Database Host:**
  - For Localhost : localhost or 127.0.0.1
  - For On Prem : IP address or Host name
  - For Cloud: IP Address or Host Name
- **Database Name:** Database Name
- **Port:** default 5433

The connection string is formatted as follows:

```
vertica+vertica_python://{username}:{password}@{host}/{database}
```

Other parameters:

- Load Balancer - Backup Host

#### YDB

The recommended connector library for [YDB](https://ydb.tech/) is
[ydb-sqlalchemy](https://pypi.org/project/ydb-sqlalchemy/).

##### Connection String

The connection string for YDB looks like this:

```
ydb://{host}:{port}/{database_name}
```

##### Protocol

You can specify `protocol` in the `Secure Extra` field at `Advanced / Security`:

```
{
    "protocol": "grpcs"
}
```

Default is `grpc`.

##### Authentication Methods

###### Static Credentials

To use `Static Credentials` you should provide `username`/`password` in the `Secure Extra` field at `Advanced / Security`:

```
{
    "credentials": {
        "username": "...",
        "password": "..."
    }
}
```

###### Access Token Credentials

To use `Access Token Credentials` you should provide `token` in the `Secure Extra` field at `Advanced / Security`:

```
{
    "credentials": {
        "token": "...",
    }
}
```

##### Service Account Credentials

To use Service Account Credentials, you should provide `service_account_json` in the `Secure Extra` field at `Advanced / Security`:

```
{
    "credentials": {
        "service_account_json": {
            "id": "...",
            "service_account_id": "...",
            "created_at": "...",
            "key_algorithm": "...",
            "public_key": "...",
            "private_key": "..."
        }
    }
}
```

#### YugabyteDB

[YugabyteDB](https://www.yugabyte.com/) is a distributed SQL database built on top of PostgreSQL.

Note that, if you're using docker compose, the
Postgres connector library [psycopg2](https://www.psycopg.org/docs/)
comes out of the box with Superset.

The connection string looks like:

```
postgresql://{username}:{password}@{host}:{port}/{database}
```

## Connecting through the UI

Here is the documentation on how to leverage the new DB Connection UI. This will provide admins the ability to enhance the UX for users who want to connect to new databases.

![db-conn-docs](https://user-images.githubusercontent.com/27827808/125499607-94e300aa-1c0f-4c60-b199-3f9de41060a3.gif)

There are now 3 steps when connecting to a database in the new UI:

Step 1: First the admin must inform superset what engine they want to connect to. This page is powered by the `/available` endpoint which pulls on the engines currently installed in your environment, so that only supported databases are shown.

Step 2: Next, the admin is prompted to enter database specific parameters. Depending on whether there is a dynamic form available for that specific engine, the admin will either see the new custom form or the legacy SQLAlchemy form. We currently have built dynamic forms for (Redshift, MySQL, Postgres, and BigQuery). The new form prompts the user for the parameters needed to connect (for example, username, password, host, port, etc.) and provides immediate feedback on errors.

Step 3: Finally, once the admin has connected to their DB using the dynamic form they have the opportunity to update any optional advanced settings.

We hope this feature will help eliminate a huge bottleneck for users to get into the application and start crafting datasets.

##### How to setup up preferred database options and images

We added a new configuration option where the admin can define their preferred databases, in order:

```python
# A list of preferred databases, in order. These databases will be
# displayed prominently in the "Add Database" dialog. You should
# use the "engine_name" attribute of the corresponding DB engine spec
# in `superset/db_engine_specs/`.
PREFERRED_DATABASES: list[str] = [
    "PostgreSQL",
    "Presto",
    "MySQL",
    "SQLite",
]
```

For copyright reasons the logos for each database are not distributed with Superset.

##### Setting images

- To set the images of your preferred database, admins must create a mapping in the `superset_text.yml` file with engine and location of the image. The image can be host locally inside your static/file directory or online (e.g. S3)

```python
DB_IMAGES:
  postgresql: "path/to/image/postgres.jpg"
  bigquery: "path/to/s3bucket/bigquery.jpg"
  snowflake: "path/to/image/snowflake.jpg"
```

##### How to add new database engines to available endpoint

Currently the new modal supports the following databases:

- Postgres
- Redshift
- MySQL
- BigQuery

When the user selects a database not in this list they will see the old dialog asking for the SQLAlchemy URI. New databases can be added gradually to the new flow. In order to support the rich configuration a DB engine spec needs to have the following attributes:

1. `parameters_schema`: a Marshmallow schema defining the parameters needed to configure the database. For Postgres this includes username, password, host, port, etc. ([see](https://github.com/apache/superset/blob/accee507c0819cd0d7bcfb5a3e1199bc81eeebf2/superset/db_engine_specs/base.py#L1309-L1320)).
2. `default_driver`: the name of the recommended driver for the DB engine spec. Many SQLAlchemy dialects support multiple drivers, but usually one is the official recommendation. For Postgres we use "psycopg2".
3. `sqlalchemy_uri_placeholder`: a string that helps the user in case they want to type the URI directly.
4. `encryption_parameters`: parameters used to build the URI when the user opts for an encrypted connection. For Postgres this is `{"sslmode": "require"}`.

In addition, the DB engine spec must implement these class methods:

- `build_sqlalchemy_uri(cls, parameters, encrypted_extra)`: this method receives the distinct parameters and builds the URI from them.
- `get_parameters_from_uri(cls, uri, encrypted_extra)`: this method does the opposite, extracting the parameters from a given URI.
- `validate_parameters(cls, parameters)`: this method is used for `onBlur` validation of the form. It should return a list of `SupersetError` indicating which parameters are missing, and which parameters are definitely incorrect ([example](https://github.com/apache/superset/blob/accee507c0819cd0d7bcfb5a3e1199bc81eeebf2/superset/db_engine_specs/base.py#L1404)).

For databases like MySQL and Postgres that use the standard format of `engine+driver://user:password@host:port/dbname` all you need to do is add the `BasicParametersMixin` to the DB engine spec, and then define the parameters 2-4 (`parameters_schema` is already present in the mixin).

For other databases you need to implement these methods yourself. The BigQuery DB engine spec is a good example of how to do that.

### Extra Database Settings

##### Deeper SQLAlchemy Integration

It is possible to tweak the database connection information using the parameters exposed by
SQLAlchemy. In the **Database edit** view, you can edit the **Extra** field as a JSON blob.

This JSON string contains extra configuration elements. The `engine_params` object gets unpacked
into the `sqlalchemy.create_engine` call, while the `metadata_params` get unpacked into the
`sqlalchemy.MetaData` call. Refer to the SQLAlchemy docs for more information.

##### Schemas

Databases like Postgres and Redshift use the **schema** as the logical entity on top of the
**database**. For Superset to connect to a specific schema, you can set the **schema** parameter in
the **Edit Tables** form (Sources > Tables > Edit record).

##### External Password Store for SQLAlchemy Connections

Superset can be configured to use an external store for database passwords. This is useful if you a
running a custom secret distribution framework and do not wish to store secrets in Superset’s meta
database.

Example: Write a function that takes a single argument of type `sqla.engine.url` and returns the
password for the given connection string. Then set `SQLALCHEMY_CUSTOM_PASSWORD_STORE` in your config
file to point to that function.

```python
def example_lookup_password(url):
    secret = <<get password from external framework>>
    return 'secret'

SQLALCHEMY_CUSTOM_PASSWORD_STORE = example_lookup_password
```

A common pattern is to use environment variables to make secrets available.
`SQLALCHEMY_CUSTOM_PASSWORD_STORE` can also be used for that purpose.

```python
def example_password_as_env_var(url):
    # assuming the uri looks like
    # mysql://localhost?superset_user:{SUPERSET_PASSWORD}
    return url.password.format(**os.environ)

SQLALCHEMY_CUSTOM_PASSWORD_STORE = example_password_as_env_var
```

##### SSL Access to Databases

You can use the `Extra` field in the **Edit Databases** form to configure SSL:

```JSON
{
    "metadata_params": {},
    "engine_params": {
          "connect_args":{
              "sslmode":"require",
              "sslrootcert": "/path/to/my/pem"
        }
     }
}
```

## Misc

### Querying across databases

Superset offers an experimental feature for querying across different databases. This is done via a special database called "Superset meta database" that uses the "superset://" SQLAlchemy URI. When using the database it's possible to query any table in any of the configured databases using the following syntax:

```sql
SELECT * FROM "database name.[[catalog.].schema].table name";
```

For example:

```sql
SELECT * FROM "examples.birth_names";
```

Spaces are allowed, but periods in the names must be replaced by `%2E`. Eg:

```sql
SELECT * FROM "Superset meta database.examples%2Ebirth_names";
```

The query above returns the same rows as `SELECT * FROM "examples.birth_names"`, and also shows that the meta database can query tables from any table — even itself!

#### Considerations

Before enabling this feature, there are a few considerations that you should have in mind. First, the meta database enforces permissions on the queried tables, so users should only have access via the database to tables that they originally have access to. Nevertheless, the meta database is a new surface for potential attacks, and bugs could allow users to see data they should not.

Second, there are performance considerations. The meta database will push any filtering, sorting, and limiting to the underlying databases, but any aggregations and joins will happen in memory in the process running the query. Because of this, it's recommended to run the database in async mode, so queries are executed in Celery workers, instead of the web workers. Additionally, it's possible to specify a hard limit on how many rows are returned from the underlying databases.

#### Enabling the meta database

To enable the Superset meta database, first you need to set the `ENABLE_SUPERSET_META_DB` feature flag to true. Then, add a new database of type "Superset meta database" with the SQLAlchemy URI "superset://".

If you enable DML in the meta database users will be able to run DML queries on underlying databases **as long as DML is also enabled in them**. This allows users to run queries that move data across databases.

Second, you might want to change the value of `SUPERSET_META_DB_LIMIT`. The default value is 1000, and defines how many are read from each database before any aggregations and joins are executed. You can also set this value `None` if you only have small tables.

Additionally, you might want to restrict the databases to with the meta database has access to. This can be done in the database configuration, under "Advanced" -> "Other" -> "ENGINE PARAMETERS" and adding:

```json
{"allowed_dbs":["Google Sheets","examples"]}
```
